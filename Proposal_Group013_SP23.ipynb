{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I1McaHdFO_Fq"
   },
   "source": [
    "# COGS 118A- Project Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zXgzWwQMO_Fu"
   },
   "source": [
    "# Project Description\n",
    "\n",
    "Suppose you are looking for a new place to live \n",
    "\n",
    "\n",
    "\n",
    "You will design and execute a machine learning project. There are a few constraints on the nature of the allowed project. \n",
    "- The problem addressed will not be a \"toy problem\" or \"common training students problem\" like mtcars, iris, palmer penguins etc.\n",
    "- The dataset will have >1k observations and >5 variables. I'd prefer more like >10k observations and >10 variables. A general rule is that if you have >100x more observations than variables, your solution will likely generalize a lot better. The goal of training a supervised machine learning model is to learn the underlying pattern in a dataset in order to generalize well to unseen data, so choosing a large dataset is very important.\n",
    "\n",
    "- The project will include a model selection and/or feature selection component where you will be looking for the best setup to maximize the performance of your ML system.\n",
    "- You will evaluate the performance of your ML system using more than one appropriate metric\n",
    "- You will be writing a report describing and discussing these accomplishments\n",
    "\n",
    "\n",
    "Feel free to delete this description section when you hand in your proposal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JG6D1J9aO_Fv"
   },
   "source": [
    "### Peer Review\n",
    "\n",
    "TBA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZwCh4-plO_Fv"
   },
   "source": [
    "# Names\n",
    "\n",
    "- Sean Perry\n",
    "- Alberto Valencia\n",
    "- Kevin Hu\n",
    "- Justin Huang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vw-LjjTO_Fv"
   },
   "source": [
    "# Abstract \n",
    "\n",
    "We want to be able to predict house prices in the United States so that potential homebuyers can get an estimate of how much they will have to spend depending on their list of home requirements. The data represents aspects of a house (location, size, num rooms etc) that a buyer may be interested in deciding on. A dataset for this can be found at https://www.kaggle.com/datasets/ahmedshahriarsakib/usa-real-estate-dataset. Our plan is to use regression to predict the house price given the features found in the dataset and use $R^2$ and MSE to evaluate our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_EOVowXGO_Fv"
   },
   "source": [
    "# Background\n",
    "\n",
    "Among other metrics, house prices are oftentimes considered in evaluating the economic performance of a particular country. On a macro and micro level, housing prices can have significant consequences–ranging from an individual deciding to purchase their first home to evaluating a country’s Gross Domestic Product<a name=\"Congressional\"></a>[<sup>[1]</sup>](#Congressionalnote). Thus, given this importance, some study has been done into home price prediction.\n",
    "This importance has led to many attempts to predict house prices by researchers dating back to the 1990s.  The article, Predicting House Prices Using Multiple Listings Data, writes about the importance of accurately predicting house prices for its importance in administering mortgages and home owner insurance <a name=\"Dubin\"></a>[<sup>[2]</sup>](#Dubinnote) . To do so, the author discusses using three algorithms: Ordinary Least Squares, Maximum likelihood, and kriging. Noting that geographical location is a difficult variable to take into account using ordinary linear regression, the author delves into each algorithm and how it is implemented, considering what model is appropriate for each scenario; in particular, the author notes that the maximum likelihood function is useful for variables which are difficult to account for. The purpose behind this is to use the correlations between the prices of nearby homes to get a more accurate estimate of a given home’s price in a more effective method given the difficulties with OLS and geography. The article then discusses a practical example from listing with Baltimore, comparing the various techniques explored in the article. In particular, the article uses grid search to maximize the parameters of the likelihood function to provide the best prediction. While the $R^2$ score for the algorithms was around $0.7$, the author did see significant improvements (as much as 65.3% improvement) using methods to determine relative housing price from nearby houses over traditional OLS. A table of values comparing the estimation sample from the prediction samples show that the estimations are fairly accurate. However, given the age of the article, it would be interesting to see if such techniques still hold up today in the world of deep networks. \n",
    "Recent research attempts to use a generalized Linear Regression in conjunction with other statistical measures as a baseline model to predict housing prices. This is done to improve the accuracy of models that attempt to predict housing price with the importance housing prices have for both a consumer and supplier as each considers the risks involved in purchasing real estate<a name=\"li\"></a>[<sup>[3]</sup>](#Linote) . In particular, the article considers controlling investment in real estate as it has historically caused long-term economic issues citing this as a motivating factor for accurately predicting housing prices. The article delves into a detailed explanation of how their model is trained and all the different parameters it considers. Although the accuracy of the model is not stated, it seems that this group use of nonparametric parameters has led to a more accurate model compared to previous models.\n",
    "With respect to the dataset we are currently using, a handful of kaggle users have made attempts to predict house price given the dataset already, with limited success. There have been 15 notebooks made with a handful using ML with varying degrees of quality and accuracy. One of the more completed notebooks is<a name=\"Masghiff\"></a>[<sup>[4]</sup>](#Masghiffnote) . This notebook looked at 5 different models (Decision Trees, Random Forests, Gradient Boosting, Ridge CV, and ElasticNetCV) with onehot encoding and scaling transformations. There was no cross validation, model selection or hyper parameter tuning. The best scoring model  was random forests with a R^2 of 0.72 and an MSE of.0.27. Our work will focus much more on model improvements (and greater dataset expansion if possible) as a result to differentiate between us and previous work. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q2OX_Ha7O_Fw"
   },
   "source": [
    "# Problem Statement\n",
    "\n",
    "Clearly describe the problem that you are solving. Avoid ambiguous words. The problem described should be well defined and should have at least one ML-relevant potential solution. Additionally, describe the problem thoroughly such that it is clear that the problem is quantifiable (the problem can be expressed in mathematical or logical terms), measurable (the problem can be measured by some metric and clearly observed), and replicable (the problem can be reproduced and occurs more than once).\n",
    "\n",
    "\n",
    "Given the number of beds, baths, arces, city, state, zip code, and the size of a given house in the United States, we want to predict the price of that house as closely to the actual price of that house as possible. This will be done using regression models (such as linear regression, random forests, etc) on the Housing Prices in Metropolitan Areas of India Kaggle Dataset with performance measured by $R^2$ and MSE (and other potential evaluation meterics for regression based problems)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IOjMskm6O_Fw"
   },
   "source": [
    "# Data\n",
    "The dataset we will use is the USA Real Estate Dataset from Kaggle found at https://www.kaggle.com/datasets/ahmedshahriarsakib/usa-real-estate-dataset. It contains 100,000 rows with 10 variables. Each obsrevation consistes of (note that * imply key variables) \n",
    "\n",
    "  1) is the house ready for sale or ready to build\n",
    "\n",
    " 2)# of beds in the house (int)*\n",
    " \n",
    " 3)# of baths in the house (int)*\n",
    "\n",
    " 4) size of the property in acres (float)*\n",
    "\n",
    " 5) city (str)*\n",
    "\n",
    " 6) state (str)*\n",
    "\n",
    " 7) zip code (int)*\n",
    "\n",
    " 8) house_size (float)*\n",
    "\n",
    " 9) previosly sold date\n",
    "\n",
    " 10) the price of the house (float) **\n",
    "\n",
    "Besides potenital onehot encoding, not many data cleaning transformations will be needed. This is a very clean dataset. The only downside is that the dataset despite being advertised as a USA real estate dataset, much of the data is from only Massachusetts and Puerto Rico. The methodolgy for how the data was scraped was detailed in the kaggle page so if we are interested in working further with this dataset, we could potentially web scrape our own data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dDCKMAe5O_Fw"
   },
   "source": [
    "# Proposed Solution\n",
    "This problem is a regression problem: Given some input features relating to a house, we predict some real valued output that is the price of that house. Therefore our proposed solution will find some optimal model for predicting housing prices.To achieve this, we will start by using various techniques and models with minimal error. The steps involved in our solution are as follows:\n",
    "\n",
    "Step 1 Data processing: We will process the housing data and handle missing values by filling them  in, doing some scaling, and encoding categorical variables if needed. This will help make sure that the data will be easily analyzed. Specifically we will at least attempt:\n",
    "\n",
    "- Scale: It may be appropriate to scale say geographical distances on a scale [0,1]. We are not sure yet how to scale but know based on our research that it may come in hand when considering geographical distances. Also, it may be useful to scale when considering similarity between housing which may be useful in predicting price. As for other features such as arce size and house size, logrithmic scaling may be appropriate to get better senses of magnitude. \n",
    "- Onehot encoding - it is possible to have categorical variables which in terms of linear regression would require one hot encoding\n",
    "\n",
    "Step 2 Model Selection: we will use all of the different regression models that we covered in class, and from there find the optimal one. These models may include:\n",
    "\n",
    "- Linear Regression:  A linear regression model seems appropriate given that we want to predict house price, say Y, given a dataset of houses and their respective attributes, say X.  In other words, given some input features relating to a house, we predict some real valued output that is the price of that house. Thus we can model this problem as $Y = Xw$\n",
    "- random forests: Each decision tree in the random forest guesses some prediction for the house price based on a random sampling of houses and features. Then we can take the mean guess of those random forests and use that as our prediction score. \n",
    "- gradient boosting: When we are training our model it is probable that there will be some sort of misclassification so it will be necessary that we take into account the error and focus on correcting it on the next iteration. Also, we are not guaranteed a smooth continuous function for our data which may require gradient descent so that we can choose our optimal value, which in this case will be the most accurate prediction.\n",
    "- Neural Network: Vectorize all the data, that will be our input. Each layer of the neural network will transform and condense the input data. The last layer of the neural network will be a single output representing some number. We can then train our model by attempting to minimize MSE between the output value and the real price of the house.\n",
    "- Other possible alternatives: Based on background research, we have noticed that regression based ML systems may struggle with understanding geologically important information. Thus we may want to look into possible methods that can handle geographic data.  \n",
    "\n",
    "Step 3 Cross Validation: After gathering the data with the models and given the size of the dataset, we can use a train test split (80/20) and cross validation for model selection.\n",
    "\n",
    "Step 4 Hyperparameter Tuning: Using techniques such as grid search, we can find out the best set of hyperparameters to be used for each model and optimize their performance.\n",
    "\n",
    "Step 5: Model Selection: The optimal model to be used to predict and be trained on will be found based on getting the lowest MSE, MAE, or highest R-Squared value. From there we have our machine learning predictor\n",
    "\n",
    "Much of this work can be easily implemented in sklearn using pandas to hold the data, numpy for extra transformations if needed, and pytorch for any neural network based model we are interested in trying out. \n",
    "\n",
    "A baseline model for this dataset, as previously discussed in the background section is a kaggle notebook which did our problem on this dataset at <a name=\"Masghiff\"></a>[<sup>[4]</sup>](#Masghiffnote). Given the original dataset did not do any kind of cross validation to conduct model selection, We can use thier pipeline for transforming the data in addition to the best model they discovered to compare against our techniques. Even if we attempt to expand the dataset via web scraping, we can still use that model for a baseline model. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n-0ncwlbO_Fw"
   },
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "\n",
    "**Mean Squared Error (MSE)**  \n",
    "MSE is defined as: $$MSE = \\frac{1}{n} \\sum_{i=1}^{n}(y_i - \\hat{y_i})^2$$\n",
    "This metric measures the average of the squared differences between predicted and actual values. This in an appropriate metric because it provides us a measurement of how accurate our predictions are with respect to the actual values. Based on our MSE value, we can quantify how close or far we are when we compare our predicted values to acutal values. The lower the MSE, the better the model is at fitting the data. \n",
    "  \n",
    "**R-squared ($R^2$)**    \n",
    "R^2 is defined as:$$R^2 = 1 - \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y_i})^2}{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}$$  \n",
    "This metric measures the fit of a regression model. This value ranges from 0 to 1. An R^2 value of 1 implies a perfect fit while a value of 0 there is no fit or relationship between the dependent and indepedent variables. This is an appropriate metric because it allows us to quantify the variance in the dependent variable in relation to the independent variable. Additionally, we can understand the fit of our data, meaning whether our data is overfit or underfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MbAFte88O_Fw"
   },
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ktjkjB7pO_Fw"
   },
   "source": [
    "There are three primarily ethics issues with our dataset 1) Lack of Informed Consent and 2) Possible collection and/or dataset bias and 3) issues with future generalizablity of the model. \n",
    "\n",
    "Since the data we are planning to use is web scraped data from publicly available websites, homeowners are unlikely to be aware of our project. Thus the rough description of their home may be included in our data analysis similar to the information available to anyone with access to realtor.com or zillow.com. Founrelty, we are unable to determine who current homeowners are with the data we have, so all other elements of a homeowners identity are hidden. However, given this we will be unable to contact these homeowners and be able to gain consent to use data about thier house. Thus leading to a lack of informed consent regrauding our project. The best solution for this issue is to allow people to message us online via github where this project is located and raise an issue with the data so we may remove their house from the analysis. \n",
    "\n",
    "We may also have a collection and/or dataset bias if we choose to use the current kaggle dataset. While the kaggle dataset advertises itself as relating to the whole USA, it is limited to only Massachusetts and Puerto Rico. This means if we wish to keep the scope of the project to the entire USA, we will be biasing our model by only considering these 2 states/terrtories. The best remedy we have for this is to gather more data, mostly likely by building our own webscraper to gather information about the houses on realtor.com (which the kaggle dataset scraped their data from) or by getting in contact with the original creator of the dataset and seeing if a more encompassing dataset can be produced. That way the location of the house can more accurately and ethically predict the price of the house. \n",
    "\n",
    "Finally there is the potential for users to be harmed by this model in the future as housing prices can change with the housing market, policy decisions etc that the model will not be able to account for given the data we have. If a user in 10 years wishes to use the model to predict what their house price will be, it has the potential to be very wrong if there is a large change in the housing market of a particular area. If that user makes a decision on that information it will likely harm their ability to search for a good house. Thus the model will likely need a pipeline to continuously update itself given a new market if put into production. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8vpauC2wO_Fx"
   },
   "source": [
    "# Team Expectations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rnH5wwB-O_Fx"
   },
   "source": [
    "* Hold in-person meetings to discuss ideas and progress\n",
    "* Communication will be handled remotely through Discord\n",
    "* Ensure minimum consensus of 3/4 members when making decisions\n",
    "* Expected individual contribution of at least 25% for each checkpoint/deliverable\n",
    "* Individual tasks will have earlier deadlines prior to submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5P_rQRbO_Fx"
   },
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zTbPXaCrO_Fx"
   },
   "source": [
    "\n",
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 5/12 | 4 PM | Determine best form of communication |  Discuss each individuals interests to find a topic for the project|\n",
    "| 5/17  | Before 11:59 PM |  Look for interesting datasets; finalize Research Proposal | Webscraping and additional data plans |\n",
    "| 5/20  |  12 PM | Finalize Web Scraper If Used | Discuss EDA plan |\n",
    "| 5/25  |  12 PM | EDA Finished | Begin planing Baseline Model |\n",
    "| 5/30  |  12 PM | Baseline Model Finshed | Prep checkpoint notebook for submission |\n",
    "| 5/31  |   Before 11:59 PM |  Checkpoint Due | Discuss model selection plans |\n",
    "| 6/10  |  12 PM | Finalize Model Selection | Plan out remaining tasks | \n",
    "| 6/14  |   Before 11:59 PM |  Project Due | NA |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ntVmqe_EO_Fx"
   },
   "source": [
    "# Footnotes\n",
    "<a name=\"Congressionalnote\"></a>1.[^](#Congressional) Congressional Research Service. (2023, January 3). Introduction to U.S. economy: Housing market - federation of American ... Introduction to U.S. Economy: Housing Market. https://sgp.fas.org/crs/misc/IF11327.pdf\n",
    "\n",
    "<a name=\"Dubinnote\"></a>2.[^](#Dubin)  Dubin, R. A. (1998). Predicting House Prices Using Multiple Listings Data. The Journal of Real Estate Finance and Economics, 17(1), 35–59. https://doi.org/10.1023/a:1007751112669\n",
    "\n",
    "<a name=\"Linote\"></a>3.[^](#li)  Li, X. (2022). Prediction and analysis of housing price based on the generalized linear regression model. Computational Intelligence and Neuroscience, 2022, 1–9. https://doi.org/10.1155/2022/3590224\n",
    "\n",
    "<a name=\"Masghiffnote\"></a>4.[^](#Masghiff) Masghiff. (2023, May 5). Predicting housing prices EDA + ml. Kaggle. https://www.kaggle.com/code/masghiff/predicting-housing-prices-eda-ml "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

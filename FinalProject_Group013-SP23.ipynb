{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118A - Final Project"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Housing Prices with a Twist\n",
    "##### Comparing Classical and Neural Networks for Simple Regression\n",
    "\n",
    "## Group members\n",
    "\n",
    "- Sean Perry\n",
    "- Alberto Valencia\n",
    "- Kevin Hu\n",
    "- Justin Huang"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "This section should be short and clearly stated. It should be a single paragraph <200 words.  It should summarize: \n",
    "- what your goal/problem is\n",
    "- what the data used represents \n",
    "- the solution/what you did\n",
    "- major results you came up with (mention how results are measured) \n",
    "\n",
    "__NB:__ this final project form is much more report-like than the proposal and the checkpoint. Think in terms of writing a paper with bits of code in the middle to make the plots/tables\n",
    "\n",
    "\n",
    "Neural Networks haven't seemed to be very popular for regression compared to more classcial machine learning methods. Interested as to why this maybe the case We set out to explore the diffrences between these classical machine learning techinques and neural networks when it comes to regression. To do this we will look at housing price prediction in India from the India Home Dataset found at https://www.kaggle.com/datasets/ruchi798/housing-prices-in-metropolitan-areas-of-india. The data represents the price of the home, the area of the home, and about the presecne of 40 diffrent kind of ammienties in the surrounding area. Towards predicting the price of the home and comparing the approches for classical and neural net based machine learning, we devided the work into two sections: classical machine learning and neural network development. For the classical machine learning, we used hyperparameter tuning via train, test, validation spilts to identify the best parameters for XX diffrent classical algorithms, then experimented with creating diffrent ensembles from each of those models to get an ideal classical machine learning algorithm for predicting home prices on this dataset. For the neueral network side, we created a variety of fairly shallow models to try and optimize *MSELoss and DO WE ADD ANOTHER LOSS HERE* and tested each on validation data to see what created the lowest loss with the simpliest model to explain the data. After trying these two approches, we compared the results of these two algorithms via $MSE$ and $R^2$ to better understand what these two approches can accomplish on thier own. We found that classical machine learning approches had larger $R^2$ but worse $MSE$ than neueral networks (ADD SPEFFIFC NUMBERS) implying that classical machine learning for regression is far more explainable than neueral networks. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Fill in the background and discuss the kind of prior work that has gone on in this research area here. **Use inline citation** to specify which references support which statements.  You can do that through HTML footnotes (demonstrated here). I used to reccommend Markdown footnotes (google is your friend) because they are simpler but recently I have had some problems with them working for me whereas HTML ones always work so far. So use the method that works for you, but do use inline citations.\n",
    "\n",
    "Here is an example of inline citation. After government genocide in the 20th century, real birds were replaced with surveillance drones designed to look just like birds<a name=\"lorenz\"></a>[<sup>[1]</sup>](#lorenznote). Use a minimum of 2 or 3 citations, but we prefer more <a name=\"admonish\"></a>[<sup>[2]</sup>](#admonishnote). You need enough citations to fully explain and back up important facts. \n",
    "\n",
    "Remeber you are trying to explain why someone would want to answer your question or why your hypothesis is in the form that you've stated. \n",
    "\n",
    "\n",
    "# Background\n",
    "\n",
    "Among other metrics, house prices are oftentimes considered in evaluating the economic performance of a particular country. On a macro and micro level, housing prices can have significant consequences–ranging from an individual deciding to purchase their first home to evaluating a country’s Gross Domestic Product<a name=\"Congressional\"></a>[<sup>[1]</sup>](#Congressionalnote). Thus, given this importance, some study has been done into home price prediction.\n",
    "This importance has led to many attempts to predict house prices by researchers dating back to the 1990s.  The article, Predicting House Prices Using Multiple Listings Data, writes about the importance of accurately predicting house prices for its importance in administering mortgages and home owner insurance <a name=\"Dubin\"></a>[<sup>[2]</sup>](#Dubinnote) . To do so, the author discusses using three algorithms: Ordinary Least Squares, Maximum likelihood, and kriging. Noting that geographical location is a difficult variable to take into account using ordinary linear regression, the author delves into each algorithm and how it is implemented, considering what model is appropriate for each scenario; in particular, the author notes that the maximum likelihood function is useful for variables which are difficult to account for. The purpose behind this is to use the correlations between the prices of nearby homes to get a more accurate estimate of a given home’s price in a more effective method given the difficulties with OLS and geography. The article then discusses a practical example from listing with Baltimore, comparing the various techniques explored in the article. In particular, the article uses grid search to maximize the parameters of the likelihood function to provide the best prediction. While the $R^2$ score for the algorithms was around $0.7$, the author did see significant improvements (as much as 65.3% improvement) using methods to determine relative housing price from nearby houses over traditional OLS. A table of values comparing the estimation sample from the prediction samples show that the estimations are fairly accurate. However, given the age of the article, it would be interesting to see if such techniques still hold up today in the world of deep networks. \n",
    "Recent research attempts to use a generalized Linear Regression in conjunction with other statistical measures as a baseline model to predict housing prices. This is done to improve the accuracy of models that attempt to predict housing price with the importance housing prices have for both a consumer and supplier as each considers the risks involved in purchasing real estate<a name=\"li\"></a>[<sup>[3]</sup>](#Linote) . In particular, the article considers controlling investment in real estate as it has historically caused long-term economic issues citing this as a motivating factor for accurately predicting housing prices. The article delves into a detailed explanation of how their model is trained and all the different parameters it considers. Although the accuracy of the model is not stated, it seems that this group use of nonparametric parameters has led to a more accurate model compared to previous models.\n",
    "With respect to the dataset we are currently using, a handful of kaggle users have made attempts to predict house price given the dataset already, with limited success. There have been 15 notebooks made with a handful using ML with varying degrees of quality and accuracy. One of the more completed notebooks is<a name=\"Masghiff\"></a>[<sup>[4]</sup>](#Masghiffnote) . This notebook looked at 5 different models (Decision Trees, Random Forests, Gradient Boosting, Ridge CV, and ElasticNetCV) with onehot encoding and scaling transformations. There was no cross validation, model selection or hyper parameter tuning. The best scoring model  was random forests with a R^2 of 0.72 and an MSE of.0.27. Our work will focus much more on model improvements (and greater dataset expansion if possible) as a result to differentiate between us and previous work. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "Clearly describe the problem that you are solving. Avoid ambiguous words. The problem described should be well defined and should have at least one ML-relevant potential solution. Additionally, describe the problem thoroughly such that it is clear that the problem is quantifiable (the problem can be expressed in mathematical or logical terms), measurable (the problem can be measured by some metric and clearly observed), and replicable (the problem can be reproduced and occurs more than once).\n",
    "\n",
    "# Problem Statement\n",
    "\n",
    "Given the size of the house, location, and the existance of nearby ammenties India, we want to predict the price of that house as closely to the actual price of that house as possible. This will be done by two approches: using regression models (such as linear regression, random forests, etc) on the dataset and using custom nerual networks built iny pytorch with performance measured by $R^2$ and MSE (and other potential evaluation meterics for regression based problems)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Detail how/where you obtained the data and cleaned it (if necessary)\n",
    "\n",
    "If the data cleaning process is very long (e.g., elaborate text processing) consider describing it briefly here in text, and moving the actual clearning process to another notebook in your repo (include a link here!).  The idea behind this approach: this is a report, and if you blow up the flow of the report to include a lot of code it makes it hard to read.\n",
    "\n",
    "Please give the following infomration for each dataset you are using\n",
    "- link/reference to obtain it\n",
    "- description of the size of the dataset (# of variables, # of observations)\n",
    "- what an observation consists of\n",
    "- what some critical variables are, how they are represented\n",
    "- any special handling, transformations, cleaning, etc you have done should be demonstrated here!\n",
    "\n",
    "\n",
    "The dataset we are loooking at can be found at https://www.kaggle.com/datasets/ruchi798/housing-prices-in-metropolitan-areas-of-india. After cleaning the dataset as shown below, we end up with 10093 rows × 41 columns. Each obsrevation consists of\n",
    "- Price (int), We are unsure extactly what the unit is since documentation about data is poor.\n",
    "- Area (int), living area inside the house,\n",
    "- Location (str), neighborhood the house is located in\n",
    "- City(Str), city house is in\n",
    "- No. of Bedrooms (int)\n",
    "- Resale(int) 0 if house is new, 1 is being resold\n",
    "- The rest of the 34 columns are ammenities an nearby attractions (1 if present, 0 is not present, stored as ints), those being: ['MaintenanceStaff', 'Gymnasium', 'SwimmingPool', 'LandscapedGardens','JoggingTrack', 'RainWaterHarvesting', 'IndoorGames', 'ShoppingMall','Intercom', 'SportsFacility', 'ATM', 'ClubHouse', 'School','24X7Security', 'PowerBackup', 'CarParking', 'StaffQuarter','Cafeteria', 'MultipurposeRoom', 'Hospital', 'WashingMachine','Gasconnection', 'AC', 'Wifi', \"Children'splayarea\", 'LiftAvailable','BED', 'Microwave', 'GolfCourse', 'TV','DiningTable', 'Sofa', 'Wardrobe', 'Refrigerator']\n",
    "\n",
    "Required transformations will be onehotencodings for location and city. Other possible transformations can look into standarizing area and/or number of bedrooms and converting the data to binary true/false for better memory space if needed.\n",
    "\n",
    "The results of this cleaning can be found at (INCLUDE DATA CLEANING LINK).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Price</th>\n",
       "      <th>Area</th>\n",
       "      <th>Location</th>\n",
       "      <th>No. of Bedrooms</th>\n",
       "      <th>Resale</th>\n",
       "      <th>MaintenanceStaff</th>\n",
       "      <th>Gymnasium</th>\n",
       "      <th>SwimmingPool</th>\n",
       "      <th>LandscapedGardens</th>\n",
       "      <th>...</th>\n",
       "      <th>BED</th>\n",
       "      <th>VaastuCompliant</th>\n",
       "      <th>Microwave</th>\n",
       "      <th>GolfCourse</th>\n",
       "      <th>TV</th>\n",
       "      <th>DiningTable</th>\n",
       "      <th>Sofa</th>\n",
       "      <th>Wardrobe</th>\n",
       "      <th>Refrigerator</th>\n",
       "      <th>City</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>6968000</td>\n",
       "      <td>1340</td>\n",
       "      <td>Nizampet</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Hyderabad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>29000000</td>\n",
       "      <td>3498</td>\n",
       "      <td>Hitech City</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Hyderabad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>6590000</td>\n",
       "      <td>1318</td>\n",
       "      <td>Manikonda</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Hyderabad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5739000</td>\n",
       "      <td>1295</td>\n",
       "      <td>Alwal</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Hyderabad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5679000</td>\n",
       "      <td>1145</td>\n",
       "      <td>Kukatpally</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Hyderabad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10088</th>\n",
       "      <td>1946</td>\n",
       "      <td>8306999</td>\n",
       "      <td>1184</td>\n",
       "      <td>Hosa Road</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Bangalore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10089</th>\n",
       "      <td>1947</td>\n",
       "      <td>4883000</td>\n",
       "      <td>655</td>\n",
       "      <td>Hosa Road</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Bangalore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10090</th>\n",
       "      <td>1948</td>\n",
       "      <td>11500000</td>\n",
       "      <td>1680</td>\n",
       "      <td>Hosa Road</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Bangalore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10091</th>\n",
       "      <td>1949</td>\n",
       "      <td>8378000</td>\n",
       "      <td>1195</td>\n",
       "      <td>Hosa Road</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Bangalore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10092</th>\n",
       "      <td>1950</td>\n",
       "      <td>5951000</td>\n",
       "      <td>820</td>\n",
       "      <td>Hosa Road</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Bangalore</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10093 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0     Price  Area     Location  No. of Bedrooms  Resale  \\\n",
       "0               0   6968000  1340     Nizampet                2       0   \n",
       "1               1  29000000  3498  Hitech City                4       0   \n",
       "2               2   6590000  1318    Manikonda                2       0   \n",
       "3               3   5739000  1295        Alwal                3       1   \n",
       "4               4   5679000  1145   Kukatpally                2       0   \n",
       "...           ...       ...   ...          ...              ...     ...   \n",
       "10088        1946   8306999  1184    Hosa Road                2       0   \n",
       "10089        1947   4883000   655    Hosa Road                1       0   \n",
       "10090        1948  11500000  1680    Hosa Road                3       0   \n",
       "10091        1949   8378000  1195    Hosa Road                2       0   \n",
       "10092        1950   5951000   820    Hosa Road                1       0   \n",
       "\n",
       "       MaintenanceStaff  Gymnasium  SwimmingPool  LandscapedGardens  ...  BED  \\\n",
       "0                   0.0        1.0           1.0                1.0  ...  0.0   \n",
       "1                   0.0        1.0           1.0                1.0  ...  0.0   \n",
       "2                   0.0        1.0           0.0                0.0  ...  0.0   \n",
       "3                   0.0        0.0           0.0                0.0  ...  0.0   \n",
       "4                   0.0        0.0           0.0                1.0  ...  0.0   \n",
       "...                 ...        ...           ...                ...  ...  ...   \n",
       "10088               0.0        1.0           1.0                1.0  ...  0.0   \n",
       "10089               0.0        1.0           1.0                1.0  ...  0.0   \n",
       "10090               0.0        1.0           1.0                1.0  ...  0.0   \n",
       "10091               0.0        1.0           1.0                1.0  ...  0.0   \n",
       "10092               0.0        1.0           1.0                1.0  ...  0.0   \n",
       "\n",
       "       VaastuCompliant  Microwave  GolfCourse   TV  DiningTable  Sofa  \\\n",
       "0                  1.0        0.0         0.0  0.0          0.0   0.0   \n",
       "1                  1.0        0.0         0.0  0.0          0.0   0.0   \n",
       "2                  0.0        0.0         0.0  0.0          0.0   0.0   \n",
       "3                  0.0        0.0         0.0  0.0          0.0   0.0   \n",
       "4                  0.0        0.0         0.0  0.0          0.0   0.0   \n",
       "...                ...        ...         ...  ...          ...   ...   \n",
       "10088              0.0        0.0         1.0  0.0          0.0   0.0   \n",
       "10089              0.0        0.0         1.0  0.0          0.0   0.0   \n",
       "10090              0.0        0.0         1.0  0.0          0.0   0.0   \n",
       "10091              0.0        0.0         1.0  0.0          0.0   0.0   \n",
       "10092              0.0        0.0         1.0  0.0          0.0   0.0   \n",
       "\n",
       "       Wardrobe  Refrigerator       City  \n",
       "0           0.0           0.0  Hyderabad  \n",
       "1           0.0           0.0  Hyderabad  \n",
       "2           0.0           0.0  Hyderabad  \n",
       "3           0.0           0.0  Hyderabad  \n",
       "4           0.0           0.0  Hyderabad  \n",
       "...         ...           ...        ...  \n",
       "10088       0.0           0.0  Bangalore  \n",
       "10089       0.0           0.0  Bangalore  \n",
       "10090       0.0           0.0  Bangalore  \n",
       "10091       0.0           0.0  Bangalore  \n",
       "10092       0.0           0.0  Bangalore  \n",
       "\n",
       "[10093 rows x 42 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = cleaned_df = pd.read_csv(\"cleaned_df.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Price', 'Area', 'Location', 'No. of Bedrooms', 'Resale',\n",
       "       'MaintenanceStaff', 'Gymnasium', 'SwimmingPool', 'LandscapedGardens',\n",
       "       'JoggingTrack', 'RainWaterHarvesting', 'IndoorGames', 'ShoppingMall',\n",
       "       'Intercom', 'SportsFacility', 'ATM', 'ClubHouse', 'School',\n",
       "       '24X7Security', 'PowerBackup', 'CarParking', 'StaffQuarter',\n",
       "       'Cafeteria', 'MultipurposeRoom', 'Hospital', 'WashingMachine',\n",
       "       'Gasconnection', 'AC', 'Wifi', 'Children'splayarea', 'LiftAvailable',\n",
       "       'BED', 'VaastuCompliant', 'Microwave', 'GolfCourse', 'TV',\n",
       "       'DiningTable', 'Sofa', 'Wardrobe', 'Refrigerator', 'City'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "In this section, clearly describe a solution to the problem. The solution should be applicable to the project domain and appropriate for the dataset(s) or input(s) given. Provide enough detail (e.g., algorithmic description and/or theoretical properties) to convince us that your solution is applicable. Make sure to describe how the solution will be tested.  \n",
    "\n",
    "If you know details already, describe how (e.g., library used, function calls) you plan to implement the solution in a way that is reproducible.\n",
    "\n",
    "If it is appropriate to the problem statement, describe a benchmark model<a name=\"sota\"></a>[<sup>[3]</sup>](#sotanote) against which your solution will be compared. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "Propose at least one evaluation metric that can be used to quantify the performance of both the benchmark model and the solution model. The evaluation metric(s) you propose should be appropriate given the context of the data, the problem statement, and the intended solution. Describe how the evaluation metric(s) are derived and provide an example of their mathematical representations (if applicable). Complex evaluation metrics should be clearly defined and quantifiable (can be expressed in mathematical or logical terms)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "You may have done tons of work on this. Not all of it belongs here. \n",
    "\n",
    "Reports should have a __narrative__. Once you've looked through all your results over the quarter, decide on one main point and 2-4 secondary points you want us to understand. Include the detailed code and analysis results of those points only; you should spend more time/code/plots on your main point than the others.\n",
    "\n",
    "If you went down any blind alleys that you later decided to not pursue, please don't abuse the TAs time by throwing in 81 lines of code and 4 plots related to something you actually abandoned.  Consider deleting things that are not important to your narrative.  If its slightly relevant to the narrative or you just want us to know you tried something, you could keep it in by summarizing the result in this report in a sentence or two, moving the actual analysis to another file in your repo, and providing us a link to that file.\n",
    "\n",
    "### Subsection 1\n",
    "\n",
    "You will likely have different subsections as you go through your report. For instance you might start with an analysis of the dataset/problem and from there you might be able to draw out the kinds of algorithms that are / aren't appropriate to tackle the solution.  Or something else completely if this isn't the way your project works.\n",
    "\n",
    "### Subsection 2\n",
    "\n",
    "Another likely section is if you are doing any feature selection through cross-validation or hand-design/validation of features/transformations of the data\n",
    "\n",
    "### Subsection 3\n",
    "\n",
    "Probably you need to describe the base model and demonstrate its performance.  Maybe you include a learning curve to show whether you have enough data to do train/validate/test split or have to go to k-folds or LOOCV or ???\n",
    "\n",
    "### Subsection 4\n",
    "\n",
    "Perhaps some exploration of the model selection (hyper-parameters) or algorithm selection task. Validation curves, plots showing the variability of perfromance across folds of the cross-validation, etc. If you're doing one, the outcome of the null hypothesis test or parsimony principle check to show how you are selecting the best model.\n",
    "\n",
    "### Subsection 5 \n",
    "\n",
    "Maybe you do model selection again, but using a different kind of metric than before?\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "### Interpreting the result\n",
    "\n",
    "OK, you've given us quite a bit of tech informaiton above, now its time to tell us what to pay attention to in all that.  Think clearly about your results, decide on one main point and 2-4 secondary points you want us to understand. Highlight HOW your results support those points.  You probably want 2-5 sentences per point.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "Are there any problems with the work?  For instance would more data change the nature of the problem? Would it be good to explore more hyperparams than you had time for?   \n",
    "\n",
    "### Ethics & Privacy\n",
    "\n",
    "If your project has obvious potential concerns with ethics or data privacy discuss that here.  Almost every ML project put into production can have ethical implications if you use your imagination. Use your imagination.\n",
    "\n",
    "Even if you can't come up with an obvious ethical concern that should be addressed, you should know that a large number of ML projects that go into producation have unintended consequences and ethical problems once in production. How will your team address these issues?\n",
    "\n",
    "Consider a tool to help you address the potential issues such as https://deon.drivendata.org\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Reiterate your main point and in just a few sentences tell us how your results support it. Mention how this work would fit in the background/context of other work in this field if you can. Suggest directions for future work if you want to."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"lorenznote\"></a>1.[^](#lorenz): Lorenz, T. (9 Dec 2021) Birds Aren’t Real, or Are They? Inside a Gen Z Conspiracy Theory. *The New York Times*. https://www.nytimes.com/2021/12/09/technology/birds-arent-real-gen-z-misinformation.html<br> \n",
    "<a name=\"admonishnote\"></a>2.[^](#admonish): Also refs should be important to the background, not some randomly chosen vaguely related stuff. Include a web link if possible in refs as above.<br>\n",
    "<a name=\"sotanote\"></a>3.[^](#sota): Perhaps the current state of the art solution such as you see on [Papers with code](https://paperswithcode.com/sota). Or maybe not SOTA, but rather a standard textbook/Kaggle solution to this kind of problem\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "#Data is saved on diffrent CSVs for each city\n",
    "#To make things easier, we can combine the data into one dataframe\n",
    "dfs = []\n",
    "for file in os.listdir(\"data\"):\n",
    "    df = pd.read_csv(os.path.join(\"data\", file))\n",
    "    df[\"City\"] = file.replace(\".csv\", \"\")\n",
    "    dfs.append(df)\n",
    "df = pd.concat(dfs)\n",
    "\n",
    "#As documented on kaggle, 9 implies that this information was not found for a home.\n",
    "#Therefore we replaced all 9s with np.nan as is standard for empty values\n",
    "\n",
    "temp = df[\"No. of Bedrooms\"].copy()\n",
    "df = df.applymap(lambda x: (np.nan if x == 9  else  x))\n",
    "df[\"No. of Bedrooms\"] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = df[~df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df.to_csv(\"cleaned_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import sklearn\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model\n",
    "\n",
    "Before implementing the baseline model, the function score_suite was created to score our model's perofmrance on both pipe score and mean square error since we wanted to detect how far we were from the accurate prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_suite(pipe, X_test, y_test):\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    return (\n",
    "        pipe.score(X_test, y_test),\n",
    "        sklearn.metrics.mean_squared_error(y_test, y_pred),\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we split the data into our train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    cleaned_df.drop(\"Price\", axis=1), \n",
    "    cleaned_df[\"Price\"], \n",
    "    test_size=0.33, \n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a score $\\approx$ .7 and mean squared error of 40692579165816.54."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# The function below performs a nested cross validation on a particular sklearn model. The flag cv_results_flag \n",
    "# accepts a boolean value which tells the function whether we want to output the models .cv_results_ or whether\n",
    "# we want the models best parameters\n",
    "def nested_cv_sklearnModels(model_param_dict, X_train, y_train, cv_results_flag):\n",
    "    inner_cv_results = {}\n",
    "    for model in tqdm(model_param_dict):\n",
    "        model_data = model_param_dict[model]\n",
    "        inner_cv = GridSearchCV(model_data[\"estimator\"],model_data[\"params\"])\n",
    "        if cv_results_flag == True:\n",
    "            inner_cv_results[model] = inner_cv.cv_results_\n",
    "        else:\n",
    "            inner_cv_results[model] = inner_cv.best_params_\n",
    "        inner_cv.fit(X_train, y_train)\n",
    "        inner_cv_results[model] = inner_cv.best_params_\n",
    "    return inner_cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# The function below performs does the exact same as the function above except is uses random search\n",
    "# cross validation instead.\n",
    "def random_cv_sklearnModels(model_param_dict, X_train, y_train):\n",
    "    inner_cv_results = {}\n",
    "    for model in tqdm(model_param_dict):\n",
    "        model_data = model_param_dict[model]\n",
    "        inner_cv = RandomizedSearchCV(model_data[\"estimator\"],model_data[\"params\"])\n",
    "        inner_cv.fit(X_train, y_train)\n",
    "        inner_cv_results[model] = inner_cv.best_params_\n",
    "    return inner_cv_results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary with each model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we initialize a dictionary including the regression models we expect to consider. The models we considered include support vector regression, decision tree regression, random forest regression, gradient boost regression, elastic net regression, and a Multi-layer Perceptron regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import ElasticNet, Ridge, SGDRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "ct = ColumnTransformer(\n",
    "    [\n",
    "        (\"Location_One_Hot\",  OneHotEncoder(handle_unknown=\"ignore\"), [\"Location\", \"City\"]),\n",
    "    ],\n",
    "    remainder=\"passthrough\"\n",
    ")\n",
    "\n",
    "model_param_dict = {}\n",
    "\n",
    "model_param_dict[\"LR\"] = {\n",
    "    \"estimator\": Pipeline([('transformers', ct), ('LR', LinearRegression())]),\n",
    "    \"params\": {\n",
    "    }  \n",
    "}    \n",
    "\n",
    "\n",
    "#https://scikit-learn.org/stable/tutorial/statistical_inference/putting_together.html\n",
    "#See above for why __\n",
    "model_param_dict[\"SVR\"] = {\n",
    "    \"estimator\": Pipeline([('transformers', ct), ('SVR', SVR())]),\n",
    "    \"params\": {\n",
    "        \"SVR__kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"],\n",
    "        \"SVR__degree\": [1,2,3,5,8,10,15,20,30],\n",
    "        \"SVR__epsilon\": [.2,.4,.6,.8,],\n",
    "        \"SVR__gamma\": [\"auto\", \"scale\"],\n",
    "        \"SVR__coef0\": [0, 1, 10, 100, 1000],\n",
    "        \"SVR__C\": [1,10,100,1000],\n",
    "    }  \n",
    "}    \n",
    "\n",
    "model_param_dict[\"DT\"] = {\n",
    "    \"estimator\": Pipeline([('transformers', ct), ('DT', DecisionTreeRegressor())]),\n",
    "    \"params\": {\n",
    "        \"DT__splitter\": ['best', 'random'],\n",
    "        \"DT__max_features\": [1,5,10,20,50,100],\n",
    "        \"DT__criterion\": [\"squared_error\", \"friedman_mse\", \"absolute_error\", \"poisson\"],\n",
    "        \"DT__max_depth\": [5, 10, 25, 50, 100, 1000],\n",
    "        #add more hyperparameters...\n",
    "        #https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor\n",
    "    }  \n",
    "}    \n",
    "\n",
    "model_param_dict[\"RandomForestRegressor\"] = {\n",
    "        \"estimator\": Pipeline([('transformers', ct), ('RFR', RandomForestRegressor())]),\n",
    "        \"params\": {\n",
    "        \"RFR__n_estimators\": ['squared_error', 'absolute_error', 'friedman_mse', 'poisson'],\n",
    "        \"RFR__n_estimators\": [1,5,10,20,50,100],\n",
    "        \"RFR__max_features\": [1,5,10,20,50,100],\n",
    "         #add more hyperparameters...\n",
    "        #https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor\n",
    "    }  \n",
    "}    \n",
    "\n",
    "\n",
    "model_param_dict[\"GradientBoostingRegressor\"] = {\n",
    "    \"estimator\":Pipeline([('transformers', ct), ('GBR', GradientBoostingRegressor())]),\n",
    "    \"params\": {\n",
    "        \"GBR__loss\":['squared_error', 'absolute_error', 'huber', 'quantile'],\n",
    "        \"GBR__n_estimators\": [1,5,10,20,50,100],\n",
    "        \"GBR__alpha\": [.05,.1,.15,.2,.25,.30,.35,.40,.45,.50,.55,.60,.65,.70,.75,.80,.85,.90,.95],\n",
    "        \"GBR__max_features\": [1,5,10,20,50,100],\n",
    "    }  \n",
    "}    \n",
    "\n",
    "model_param_dict[\"LassoRegression\"] = {\n",
    "    \"estimator\":Pipeline([('transformers', ct), ('LAS', Lasso())]),\n",
    "    \"params\": {\n",
    "        \"LAS__alpha\":[.05,.1,.15,.2,.25,.30,.35,.40,.45,.50,.55,.60,.65,.70,.75,.80,.85,.90,.95],\n",
    "    }  \n",
    "}    \n",
    "\n",
    "model_param_dict[\"ElasticNet\"] = {\n",
    "    \"estimator\":Pipeline([('transformers', ct), ('EL', ElasticNet())]),\n",
    "    \"params\": {\n",
    "        \"EL__alpha\":[.05,.1,.15,.2,.25,.30,.35,.40,.45,.50,.55,.60,.65,.70,.75,.80,.85,.90,.95],\n",
    "        \"EL__l1_ratio\":[.1,.30,.50,.70,.90],\n",
    "\n",
    "    }  \n",
    "}    \n",
    "\n",
    "model_param_dict[\"MultiLayerPerceptron\"] = {\n",
    "    \"estimator\": Pipeline([('transformers', ct), ('MLP', MLPRegressor(random_state=1, max_iter=50))]),\n",
    "    \"params\": {\n",
    "        \"MLP__activation\" :['identity', 'logistic', 'tanh', 'relu'],\n",
    "        \"MLP__solver\": ['lbfgs','sgd','adam'],\n",
    "        \"MLP__alpha\" : [.01,.001,.0001]\n",
    "    }  \n",
    "}  \n",
    "\n",
    "model_param_dict[\"BayesianRidgeRegression\"] = {\n",
    "    \"estimator\": Pipeline([('transformers', ct), ('BRR', BayesianRidge())]),\n",
    "    \"params\": {\n",
    "        \"BRR__alpha_1\": [1e-6, 1e-5, 1e-4, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0],\n",
    "        \"BRR__alpha_2\": [1e-6, 1e-5, 1e-4, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0],\n",
    "        \"BRR__lambda_1\": [1e-6, 1e-5, 1e-4, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0],\n",
    "        \"BRR__lambda_2\": [1e-6, 1e-5, 1e-4, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0],\n",
    "        \"BRR__tol\": [1e-4, 1e-5, 1e-6],\n",
    "    }  \n",
    "}\n",
    "\n",
    "model_param_dict[\"RidgeRegression\"] = {\n",
    "    \"estimator\": Pipeline([('transformers', ct), ('RR', Ridge())]),\n",
    "    \"params\": {\n",
    "        \"RR__alpha\":  [.05, .1, .15, .2, .25, .30, .35, .40, .45, .50, .55, .60, .65, .70, .75, .80, .85, .90, .95],\n",
    "    }  \n",
    "}\n",
    "\n",
    "model_param_dict[\"SGDRegressor\"] = {\n",
    "    \"estimator\": Pipeline([('transformers', ct), ('SGD', SGDRegressor())]),\n",
    "    \"params\": {\n",
    "        \"SGD__loss\": ['squared_error', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive'],\n",
    "        \"SGD__penalty\": [None, 'l1', 'l2', 'elasticnet'],\n",
    "        \"SGD__alpha\": [.05, .1, .15, .2, .25, .30, .35, .40, .45, .50, .55, .60, .65, .70, .75, .80, .85, .90, .95],\n",
    "        \"SGD__l1_ratio\": [.1, .30, .50, .70, .90],\n",
    "    }  \n",
    "}\n",
    "\n",
    "   \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell is used to test the hyperparameters of a specific model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running grid search and finding the best parameters, I documented them below.\n",
    "\n",
    "<ol>\n",
    "  <li>LR: {} </li>\n",
    "  <li>'SVR__C': 1000, 'SVR__coef0': 0, 'SVR__degree': 1, 'SVR__epsilon': 0.8</li>\n",
    "  <li>'DT__criterion': 'poisson','DT__max_depth': 1000,'DT__max_features': 100,'DT__splitter': 'random'</li>\n",
    "  <li>'RandomForestRegressor': 'RFR__max_features': 100, 'RFR__n_estimators': 50</li>\n",
    "  <li>'GradientBoostingRegressor': 'GBR__alpha': 0.55,'GBR__loss': 'squared_error','GBR__max_features': 100 'GBR__n_estimators': 100</li>\n",
    "  <li>'LasRegressor': 'LAS__alpha': 0.35 </li>\n",
    "  <li>ElasticNet: 'EL__alpha': 0.05, 'EL__l1_ratio': 0.9</li>\n",
    "  <li>{'RR__alpha': 0.95}</li>\n",
    "  <li>{'SGD__alpha': 0.45,\n",
    " 'SGD__l1_ratio': 0.3,\n",
    " 'SGD__loss': 'epsilon_insensitive',\n",
    " 'SGD__penalty': None}</li>\n",
    "</ol>\n",
    "\n",
    "Note while we considered using multi-layer perceptron and lasso regression, when running these models it did not always converge so we will refrain from using it. In particular, we were not even able to run grid search for lasso regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run each model on its best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LinearRegression': [0.7011333963063829, 40692579165816.54],\n",
       " 'SupportVectorRegression': [0.4720627469937472, 71881997510046.56],\n",
       " 'DecisionTree': [0.42007157701883857, 78960924275300.88],\n",
       " 'RandomForestRegressor': [0.7660770700011927, 31850087062368.96],\n",
       " 'GradientBoostingRegressor': [0.698372662697452, 41068470515163.555],\n",
       " 'ElasticNet': [0.6691158333931888, 45051972946987.58],\n",
       " 'RidgeRegression': [0.481096774854993, 70651957454093.05],\n",
       " 'SGDRegressor': [-0.07728533125393477, 146679214353316.12]}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#We create a new dictionary of the useful models and their optimal hyperparameters\n",
    "\n",
    "models_to_test = {}\n",
    "models_to_test['LinearRegression'] = Pipeline([('transformers', ct), ('LR', LinearRegression())])\n",
    "models_to_test['SupportVectorRegression'] = Pipeline([('transformers', ct), ('SVR', SVR(kernel='linear',C=1000,degree=1,epsilon=.8))])\n",
    "models_to_test['DecisionTree'] = Pipeline([('transformers', ct), ('DT', DecisionTreeRegressor(criterion='poisson', max_depth=1000,splitter='random',max_features=100))])\n",
    "models_to_test['RandomForestRegressor'] = Pipeline([('transformers', ct), ('RFR', RandomForestRegressor(max_features=100, n_estimators=50))])\n",
    "models_to_test['GradientBoostingRegressor'] = Pipeline([('transformers', ct), ('GBR', GradientBoostingRegressor(alpha=.55,loss='squared_error',max_features=100, n_estimators=100))])\n",
    "models_to_test[\"ElasticNet\"] = Pipeline([('transformers', ct), ('EN', ElasticNet(alpha=.05,l1_ratio=.9))])\n",
    "models_to_test[\"RidgeRegression\"]= Pipeline([('transformers', ct), ('RR', Ridge(alpha=.95))])\n",
    "models_to_test[\"SGDRegressor\"] = Pipeline([('transformers', ct), ('SGD', SGDRegressor(alpha=.45,l1_ratio=.30,loss='epsilon_insensitive'))])\n",
    "\n",
    "model_result_dict = {}\n",
    "for model in models_to_test:\n",
    "    curr_model = models_to_test[model]\n",
    "    curr_model.fit(X_train,y_train)\n",
    "    score,mse_error = score_suite(curr_model,X_test, y_test)\n",
    "    model_result_dict[model] = [score,mse_error]\n",
    "\n",
    "model_result_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the scores were not too favorable, and both the scores of support vector regression and decision tree was less than 50% we decided to create an ensemble. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7741758625955112, 30747385205691.523)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimators = []\n",
    "for res in model_result_dict:\n",
    "    if model_result_dict[res][0] > .5:\n",
    "        estimators.append((res,models_to_test[res]))\n",
    "\n",
    "ensemble = VotingRegressor(estimators=estimators)\n",
    "ensemble = ensemble.fit(X_train, y_train)\n",
    "score_suite(ensemble,X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that we achieve a score $\\approx$ .78 and a mean squared error less than each respective model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

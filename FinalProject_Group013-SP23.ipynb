{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118A - Final Project"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Housing Prices with a Twist\n",
    "##### Comparing Classical and Neural Networks for Simple Regression\n",
    "\n",
    "## Group members\n",
    "\n",
    "- Sean Perry\n",
    "- Alberto Valencia\n",
    "- Kevin Hu\n",
    "- Justin Huang"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "This section should be short and clearly stated. It should be a single paragraph <200 words.  It should summarize: \n",
    "- what your goal/problem is\n",
    "- what the data used represents \n",
    "- the solution/what you did\n",
    "- major results you came up with (mention how results are measured) \n",
    "\n",
    "__NB:__ this final project form is much more report-like than the proposal and the checkpoint. Think in terms of writing a paper with bits of code in the middle to make the plots/tables\n",
    "\n",
    "\n",
    "Neural Networks haven't seemed to be very popular for regression compared to more classcial machine learning methods. Interested as to why this maybe the case We set out to explore the diffrences between these classical machine learning techinques and neural networks when it comes to regression. To do this we will look at housing price prediction in India from the India Home Dataset found at https://www.kaggle.com/datasets/ruchi798/housing-prices-in-metropolitan-areas-of-india. The data represents the price of the home, the area of the home, and about the presecne of 40 diffrent kind of ammienties in the surrounding area. Towards predicting the price of the home and comparing the approches for classical and neural net based machine learning, we devided the work into two sections: classical machine learning and neural network development. For the classical machine learning, we used hyperparameter tuning via train, test, validation spilts to identify the best parameters for XX diffrent classical algorithms, then experimented with creating diffrent ensembles from each of those models to get an ideal classical machine learning algorithm for predicting home prices on this dataset. For the neueral network side, we created a variety of fairly shallow models to try and optimize *MSELoss and DO WE ADD ANOTHER LOSS HERE* and tested each on validation data to see what created the lowest loss with the simpliest model to explain the data. After trying these two approches, we compared the results of these two algorithms via $MSE$ and $R^2$ to better understand what these two approches can accomplish on thier own. We found that classical machine learning approches had larger $R^2$ but worse $MSE$ than neueral networks (ADD SPEFFIFC NUMBERS) implying that classical machine learning for regression is far more explainable than neueral networks. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Fill in the background and discuss the kind of prior work that has gone on in this research area here. **Use inline citation** to specify which references support which statements.  You can do that through HTML footnotes (demonstrated here). I used to reccommend Markdown footnotes (google is your friend) because they are simpler but recently I have had some problems with them working for me whereas HTML ones always work so far. So use the method that works for you, but do use inline citations.\n",
    "\n",
    "Here is an example of inline citation. After government genocide in the 20th century, real birds were replaced with surveillance drones designed to look just like birds<a name=\"lorenz\"></a>[<sup>[1]</sup>](#lorenznote). Use a minimum of 2 or 3 citations, but we prefer more <a name=\"admonish\"></a>[<sup>[2]</sup>](#admonishnote). You need enough citations to fully explain and back up important facts. \n",
    "\n",
    "Remeber you are trying to explain why someone would want to answer your question or why your hypothesis is in the form that you've stated. \n",
    "\n",
    "\n",
    "# Background\n",
    "\n",
    "Among other metrics, house prices are oftentimes considered in evaluating the economic performance of a particular country. On a macro and micro level, housing prices can have significant consequences–ranging from an individual deciding to purchase their first home to evaluating a country’s Gross Domestic Product<a name=\"Congressional\"></a>[<sup>[1]</sup>](#Congressionalnote). Thus, given this importance, some study has been done into home price prediction.\n",
    "This importance has led to many attempts to predict house prices by researchers dating back to the 1990s.  The article, Predicting House Prices Using Multiple Listings Data, writes about the importance of accurately predicting house prices for its importance in administering mortgages and home owner insurance <a name=\"Dubin\"></a>[<sup>[2]</sup>](#Dubinnote) . To do so, the author discusses using three algorithms: Ordinary Least Squares, Maximum likelihood, and kriging. Noting that geographical location is a difficult variable to take into account using ordinary linear regression, the author delves into each algorithm and how it is implemented, considering what model is appropriate for each scenario; in particular, the author notes that the maximum likelihood function is useful for variables which are difficult to account for. The purpose behind this is to use the correlations between the prices of nearby homes to get a more accurate estimate of a given home’s price in a more effective method given the difficulties with OLS and geography. The article then discusses a practical example from listing with Baltimore, comparing the various techniques explored in the article. In particular, the article uses grid search to maximize the parameters of the likelihood function to provide the best prediction. While the $R^2$ score for the algorithms was around $0.7$, the author did see significant improvements (as much as 65.3% improvement) using methods to determine relative housing price from nearby houses over traditional OLS. A table of values comparing the estimation sample from the prediction samples show that the estimations are fairly accurate. However, given the age of the article, it would be interesting to see if such techniques still hold up today in the world of deep networks. \n",
    "Recent research attempts to use a generalized Linear Regression in conjunction with other statistical measures as a baseline model to predict housing prices. This is done to improve the accuracy of models that attempt to predict housing price with the importance housing prices have for both a consumer and supplier as each considers the risks involved in purchasing real estate<a name=\"li\"></a>[<sup>[3]</sup>](#Linote) . In particular, the article considers controlling investment in real estate as it has historically caused long-term economic issues citing this as a motivating factor for accurately predicting housing prices. The article delves into a detailed explanation of how their model is trained and all the different parameters it considers. Although the accuracy of the model is not stated, it seems that this group use of nonparametric parameters has led to a more accurate model compared to previous models.\n",
    "With respect to the dataset we are currently using, a handful of kaggle users have made attempts to predict house price given the dataset already, with limited success. There have been 15 notebooks made with a handful using ML with varying degrees of quality and accuracy. One of the more completed notebooks is<a name=\"Masghiff\"></a>[<sup>[4]</sup>](#Masghiffnote) . This notebook looked at 5 different models (Decision Trees, Random Forests, Gradient Boosting, Ridge CV, and ElasticNetCV) with onehot encoding and scaling transformations. There was no cross validation, model selection or hyper parameter tuning. The best scoring model  was random forests with a R^2 of 0.72 and an MSE of.0.27. Our work will focus much more on model improvements (and greater dataset expansion if possible) as a result to differentiate between us and previous work. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "Clearly describe the problem that you are solving. Avoid ambiguous words. The problem described should be well defined and should have at least one ML-relevant potential solution. Additionally, describe the problem thoroughly such that it is clear that the problem is quantifiable (the problem can be expressed in mathematical or logical terms), measurable (the problem can be measured by some metric and clearly observed), and replicable (the problem can be reproduced and occurs more than once).\n",
    "\n",
    "# Problem Statement\n",
    "\n",
    "Given the size of the house, location, and the existance of nearby ammenties India, we want to predict the price of that house as closely to the actual price of that house as possible. This will be done by two approches: using regression models (such as linear regression, random forests, etc) on the dataset and using custom nerual networks built iny pytorch with performance measured by $R^2$ and MSE (and other potential evaluation meterics for regression based problems)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Detail how/where you obtained the data and cleaned it (if necessary)\n",
    "\n",
    "If the data cleaning process is very long (e.g., elaborate text processing) consider describing it briefly here in text, and moving the actual clearning process to another notebook in your repo (include a link here!).  The idea behind this approach: this is a report, and if you blow up the flow of the report to include a lot of code it makes it hard to read.\n",
    "\n",
    "Please give the following infomration for each dataset you are using\n",
    "- link/reference to obtain it\n",
    "- description of the size of the dataset (# of variables, # of observations)\n",
    "- what an observation consists of\n",
    "- what some critical variables are, how they are represented\n",
    "- any special handling, transformations, cleaning, etc you have done should be demonstrated here!\n",
    "\n",
    "\n",
    "The dataset we are loooking at can be found at https://www.kaggle.com/datasets/ruchi798/housing-prices-in-metropolitan-areas-of-india. After cleaning the dataset as shown below, we end up with 10093 rows × 41 columns. Each obsrevation consists of\n",
    "- Price (int), We are unsure extactly what the unit is since documentation about data is poor.\n",
    "- Area (int), living area inside the house,\n",
    "- Location (str), neighborhood the house is located in\n",
    "- City(Str), city house is in\n",
    "- No. of Bedrooms (int)\n",
    "- Resale(int) 0 if house is new, 1 is being resold\n",
    "- The rest of the 34 columns are ammenities an nearby attractions (1 if present, 0 is not present, stored as ints), those being: ['MaintenanceStaff', 'Gymnasium', 'SwimmingPool', 'LandscapedGardens','JoggingTrack', 'RainWaterHarvesting', 'IndoorGames', 'ShoppingMall','Intercom', 'SportsFacility', 'ATM', 'ClubHouse', 'School','24X7Security', 'PowerBackup', 'CarParking', 'StaffQuarter','Cafeteria', 'MultipurposeRoom', 'Hospital', 'WashingMachine','Gasconnection', 'AC', 'Wifi', \"Children'splayarea\", 'LiftAvailable','BED', 'Microwave', 'GolfCourse', 'TV','DiningTable', 'Sofa', 'Wardrobe', 'Refrigerator']\n",
    "\n",
    "Required transformations will be onehotencodings for location and city. Other possible transformations can look into standarizing area and/or number of bedrooms and converting the data to binary true/false for better memory space if needed.\n",
    "\n",
    "The results of this cleaning can be found at (INCLUDE DATA CLEANING LINK).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Area</th>\n",
       "      <th>Location</th>\n",
       "      <th>No. of Bedrooms</th>\n",
       "      <th>Resale</th>\n",
       "      <th>MaintenanceStaff</th>\n",
       "      <th>Gymnasium</th>\n",
       "      <th>SwimmingPool</th>\n",
       "      <th>LandscapedGardens</th>\n",
       "      <th>JoggingTrack</th>\n",
       "      <th>...</th>\n",
       "      <th>BED</th>\n",
       "      <th>VaastuCompliant</th>\n",
       "      <th>Microwave</th>\n",
       "      <th>GolfCourse</th>\n",
       "      <th>TV</th>\n",
       "      <th>DiningTable</th>\n",
       "      <th>Sofa</th>\n",
       "      <th>Wardrobe</th>\n",
       "      <th>Refrigerator</th>\n",
       "      <th>City</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30000000</td>\n",
       "      <td>3340</td>\n",
       "      <td>JP Nagar Phase 1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Bangalore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7888000</td>\n",
       "      <td>1045</td>\n",
       "      <td>Dasarahalli on Tumkur Road</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Bangalore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4866000</td>\n",
       "      <td>1179</td>\n",
       "      <td>Kannur on Thanisandra Main Road</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Bangalore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8358000</td>\n",
       "      <td>1675</td>\n",
       "      <td>Doddanekundi</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Bangalore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6845000</td>\n",
       "      <td>1670</td>\n",
       "      <td>Kengeri</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Bangalore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26637</th>\n",
       "      <td>62000000</td>\n",
       "      <td>1450</td>\n",
       "      <td>Worli</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Mumbai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26638</th>\n",
       "      <td>2500000</td>\n",
       "      <td>540</td>\n",
       "      <td>Virar East</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Mumbai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26639</th>\n",
       "      <td>19000000</td>\n",
       "      <td>1267</td>\n",
       "      <td>Belapur</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Mumbai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26640</th>\n",
       "      <td>14900000</td>\n",
       "      <td>1245</td>\n",
       "      <td>Airoli</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Mumbai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26641</th>\n",
       "      <td>14000000</td>\n",
       "      <td>1183</td>\n",
       "      <td>Airoli</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Mumbai</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10093 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Price  Area                         Location  No. of Bedrooms  \\\n",
       "0      30000000  3340                 JP Nagar Phase 1                4   \n",
       "1       7888000  1045       Dasarahalli on Tumkur Road                2   \n",
       "2       4866000  1179  Kannur on Thanisandra Main Road                2   \n",
       "3       8358000  1675                     Doddanekundi                3   \n",
       "4       6845000  1670                          Kengeri                3   \n",
       "...         ...   ...                              ...              ...   \n",
       "26637  62000000  1450                            Worli                3   \n",
       "26638   2500000   540                       Virar East                1   \n",
       "26639  19000000  1267                          Belapur                3   \n",
       "26640  14900000  1245                           Airoli                2   \n",
       "26641  14000000  1183                           Airoli                2   \n",
       "\n",
       "       Resale  MaintenanceStaff  Gymnasium  SwimmingPool  LandscapedGardens  \\\n",
       "0           0               1.0        1.0           1.0                1.0   \n",
       "1           0               0.0        1.0           1.0                1.0   \n",
       "2           0               0.0        1.0           1.0                1.0   \n",
       "3           0               0.0        0.0           0.0                0.0   \n",
       "4           0               1.0        1.0           1.0                1.0   \n",
       "...       ...               ...        ...           ...                ...   \n",
       "26637       0               1.0        1.0           1.0                1.0   \n",
       "26638       0               0.0        0.0           0.0                0.0   \n",
       "26639       1               0.0        1.0           1.0                1.0   \n",
       "26640       0               0.0        0.0           0.0                0.0   \n",
       "26641       0               0.0        1.0           1.0                1.0   \n",
       "\n",
       "       JoggingTrack  ...  BED  VaastuCompliant  Microwave  GolfCourse   TV  \\\n",
       "0               1.0  ...  0.0              0.0        0.0         0.0  0.0   \n",
       "1               1.0  ...  0.0              1.0        0.0         0.0  0.0   \n",
       "2               1.0  ...  0.0              0.0        0.0         0.0  0.0   \n",
       "3               0.0  ...  0.0              0.0        0.0         0.0  0.0   \n",
       "4               1.0  ...  0.0              0.0        0.0         0.0  0.0   \n",
       "...             ...  ...  ...              ...        ...         ...  ...   \n",
       "26637           1.0  ...  0.0              1.0        0.0         1.0  0.0   \n",
       "26638           0.0  ...  0.0              0.0        0.0         0.0  0.0   \n",
       "26639           1.0  ...  0.0              1.0        0.0         0.0  0.0   \n",
       "26640           0.0  ...  0.0              0.0        0.0         0.0  0.0   \n",
       "26641           1.0  ...  0.0              1.0        0.0         0.0  0.0   \n",
       "\n",
       "       DiningTable  Sofa  Wardrobe  Refrigerator       City  \n",
       "0              0.0   0.0       0.0           0.0  Bangalore  \n",
       "1              0.0   0.0       0.0           0.0  Bangalore  \n",
       "2              0.0   0.0       0.0           0.0  Bangalore  \n",
       "3              0.0   0.0       0.0           0.0  Bangalore  \n",
       "4              0.0   0.0       0.0           0.0  Bangalore  \n",
       "...            ...   ...       ...           ...        ...  \n",
       "26637          0.0   0.0       0.0           0.0     Mumbai  \n",
       "26638          0.0   0.0       0.0           0.0     Mumbai  \n",
       "26639          0.0   0.0       0.0           0.0     Mumbai  \n",
       "26640          0.0   0.0       0.0           0.0     Mumbai  \n",
       "26641          0.0   0.0       0.0           0.0     Mumbai  \n",
       "\n",
       "[10093 rows x 41 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = cleaned_df = pd.read_csv(\"cleaned_df.csv\", index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10093, 41)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Price', 'Area', 'Location', 'No. of Bedrooms', 'Resale',\n",
       "       'MaintenanceStaff', 'Gymnasium', 'SwimmingPool',\n",
       "       'LandscapedGardens', 'JoggingTrack', 'RainWaterHarvesting',\n",
       "       'IndoorGames', 'ShoppingMall', 'Intercom', 'SportsFacility', 'ATM',\n",
       "       'ClubHouse', 'School', '24X7Security', 'PowerBackup', 'CarParking',\n",
       "       'StaffQuarter', 'Cafeteria', 'MultipurposeRoom', 'Hospital',\n",
       "       'WashingMachine', 'Gasconnection', 'AC', 'Wifi',\n",
       "       \"Children'splayarea\", 'LiftAvailable', 'BED', 'VaastuCompliant',\n",
       "       'Microwave', 'GolfCourse', 'TV', 'DiningTable', 'Sofa', 'Wardrobe',\n",
       "       'Refrigerator', 'City'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "# Proposed Solution\n",
    "\n",
    "This problem is a regression problem: Given some input features relating to a house, we predict some real valued output that is the price of that house. Therefore our proposed solution will find some optimal model for predicting housing prices.To achieve this, we will start by using various techniques and models with minimal error. The steps involved in our solution are as follows:\n",
    "\n",
    "Step 1 Data processing: We will process the housing data and handle missing values by filling them  in, doing some scaling, and encoding categorical variables if needed. This will help make sure that the data will be easily analyzed. Specifically we will at least attempt:\n",
    "\n",
    "- Scale: It may be appropriate to scale say geographical distances on a scale [0,1]. We are not sure yet how to scale but know based on our research that it may come in hand when considering geographical distances. Also, it may be useful to scale when considering similarity between housing which may be useful in predicting price. As for other features such as arce size and house size, logrithmic scaling may be appropriate to get better senses of magnitude. \n",
    "- Onehot encoding - it is possible to have categorical variables which in terms of linear regression would require one hot encoding\n",
    "\n",
    "Step 2 Model Selection: we will use all of the different regression models that we covered in class, and from there find the optimal one. These models may include:\n",
    "\n",
    "- Linear Regression:  A linear regression model seems appropriate given that we want to predict house price, say Y, given a dataset of houses and their respective attributes, say X.  In other words, given some input features relating to a house, we predict some real valued output that is the price of that house. Thus we can model this problem as $Y = Xw$\n",
    "- random forests: Each decision tree in the random forest guesses some prediction for the house price based on a random sampling of houses and features. Then we can take the mean guess of those random forests and use that as our prediction score. \n",
    "- gradient boosting: When we are training our model it is probable that there will be some sort of misclassification so it will be necessary that we take into account the error and focus on correcting it on the next iteration. Also, we are not guaranteed a smooth continuous function for our data which may require gradient descent so that we can choose our optimal value, which in this case will be the most accurate prediction.\n",
    "- Neural Network: Vectorize all the data, that will be our input. Each layer of the neural network will transform and condense the input data. The last layer of the neural network will be a single output representing some number. We can then train our model by attempting to minimize MSE between the output value and the real price of the house.\n",
    "- Other possible alternatives: Based on background research, we have noticed that regression based ML systems may struggle with understanding geologically important information. Thus we may want to look into possible methods that can handle geographic data.  \n",
    "\n",
    "Step 3 Cross Validation: After gathering the data with the models and given the size of the dataset, we can use a train test split (80/20) and cross validation for model selection.\n",
    "\n",
    "Step 4 Hyperparameter Tuning: Using techniques such as grid search, we can find out the best set of hyperparameters to be used for each model and optimize their performance.\n",
    "\n",
    "Step 5: Model Selection: The optimal model to be used to predict and be trained on will be found based on getting the lowest MSE, MAE, or highest R-Squared value. From there we have our machine learning predictor\n",
    "\n",
    "Much of this work can be easily implemented in sklearn using pandas to hold the data, numpy for extra transformations if needed, and pytorch for any neural network based model we are interested in trying out. \n",
    "\n",
    "A baseline model for this dataset, as previously discussed in the background section is a kaggle notebook which did our problem on this dataset at <a name=\"Masghiff\"></a>[<sup>[4]</sup>](#Masghiffnote). Given the original dataset did not do any kind of cross validation to conduct model selection, We can use thier pipeline for transforming the data in addition to the best model they discovered to compare against our techniques. Even if we attempt to expand the dataset via web scraping, we can still use that model for a baseline model. \n",
    "\n",
    "# NOTE WE SHOULD UPDATE THIS SECTION FOR NEW FINAL PROEJCT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "\n",
    "**Mean Squared Error (MSE)**  \n",
    "MSE is defined as: $$MSE = \\frac{1}{n} \\sum_{i=1}^{n}(y_i - \\hat{y_i})^2$$\n",
    "This metric measures the average of the squared differences between predicted and actual values. This in an appropriate metric because it provides us a measurement of how accurate our predictions are with respect to the actual values. Based on our MSE value, we can quantify how close or far we are when we compare our predicted values to acutal values. The lower the MSE, the better the model is at fitting the data. \n",
    "  \n",
    "**R-squared ($R^2$)**    \n",
    "R^2 is defined as:$$R^2 = 1 - \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y_i})^2}{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}$$  \n",
    "This metric measures the fit of a regression model. This value ranges from 0 to 1. An R^2 value of 1 implies a perfect fit while a value of 0 there is no fit or relationship between the dependent and indepedent variables. This is an appropriate metric because it allows us to quantify the variance in the dependent variable in relation to the independent variable. Additionally, we can understand the fit of our data, meaning whether our data is overfit or underfit."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "You may have done tons of work on this. Not all of it belongs here. \n",
    "\n",
    "Reports should have a __narrative__. Once you've looked through all your results over the quarter, decide on one main point and 2-4 secondary points you want us to understand. Include the detailed code and analysis results of those points only; you should spend more time/code/plots on your main point than the others.\n",
    "\n",
    "If you went down any blind alleys that you later decided to not pursue, please don't abuse the TAs time by throwing in 81 lines of code and 4 plots related to something you actually abandoned.  Consider deleting things that are not important to your narrative.  If its slightly relevant to the narrative or you just want us to know you tried something, you could keep it in by summarizing the result in this report in a sentence or two, moving the actual analysis to another file in your repo, and providing us a link to that file.\n",
    "\n",
    "### Subsection 1\n",
    "\n",
    "You will likely have different subsections as you go through your report. For instance you might start with an analysis of the dataset/problem and from there you might be able to draw out the kinds of algorithms that are / aren't appropriate to tackle the solution.  Or something else completely if this isn't the way your project works.\n",
    "\n",
    "### Subsection 2\n",
    "\n",
    "Another likely section is if you are doing any feature selection through cross-validation or hand-design/validation of features/transformations of the data\n",
    "\n",
    "### Subsection 3\n",
    "\n",
    "Probably you need to describe the base model and demonstrate its performance.  Maybe you include a learning curve to show whether you have enough data to do train/validate/test split or have to go to k-folds or LOOCV or ???\n",
    "\n",
    "### Subsection 4\n",
    "\n",
    "Perhaps some exploration of the model selection (hyper-parameters) or algorithm selection task. Validation curves, plots showing the variability of perfromance across folds of the cross-validation, etc. If you're doing one, the outcome of the null hypothesis test or parsimony principle check to show how you are selecting the best model.\n",
    "\n",
    "### Subsection 5 \n",
    "\n",
    "Maybe you do model selection again, but using a different kind of metric than before?\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "### Interpreting the result\n",
    "\n",
    "OK, you've given us quite a bit of tech informaiton above, now its time to tell us what to pay attention to in all that.  Think clearly about your results, decide on one main point and 2-4 secondary points you want us to understand. Highlight HOW your results support those points.  You probably want 2-5 sentences per point.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "Are there any problems with the work?  For instance would more data change the nature of the problem? Would it be good to explore more hyperparams than you had time for?   \n",
    "\n",
    "### Ethics & Privacy\n",
    "\n",
    "If your project has obvious potential concerns with ethics or data privacy discuss that here.  Almost every ML project put into production can have ethical implications if you use your imagination. Use your imagination.\n",
    "\n",
    "Even if you can't come up with an obvious ethical concern that should be addressed, you should know that a large number of ML projects that go into producation have unintended consequences and ethical problems once in production. How will your team address these issues?\n",
    "\n",
    "Consider a tool to help you address the potential issues such as https://deon.drivendata.org\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Reiterate your main point and in just a few sentences tell us how your results support it. Mention how this work would fit in the background/context of other work in this field if you can. Suggest directions for future work if you want to."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"lorenznote\"></a>1.[^](#lorenz): Lorenz, T. (9 Dec 2021) Birds Aren’t Real, or Are They? Inside a Gen Z Conspiracy Theory. *The New York Times*. https://www.nytimes.com/2021/12/09/technology/birds-arent-real-gen-z-misinformation.html<br> \n",
    "<a name=\"admonishnote\"></a>2.[^](#admonish): Also refs should be important to the background, not some randomly chosen vaguely related stuff. Include a web link if possible in refs as above.<br>\n",
    "<a name=\"sotanote\"></a>3.[^](#sota): Perhaps the current state of the art solution such as you see on [Papers with code](https://paperswithcode.com/sota). Or maybe not SOTA, but rather a standard textbook/Kaggle solution to this kind of problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

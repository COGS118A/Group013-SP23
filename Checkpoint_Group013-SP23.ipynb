{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118A - Project Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "- Sean Perry\n",
    "- Alberto Valencia\n",
    "- Kevin Hu\n",
    "- Justin Huang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "\n",
    "We want to be able to predict house prices in the United States so that potential homebuyers can get an estimate of how much they will have to spend depending on their list of home requirements. The data represents aspects of a house (location, size, num rooms etc) that a buyer may be interested in deciding on. A dataset for this can be found at https://www.kaggle.com/datasets/ruchi798/housing-prices-in-metropolitan-areas-of-india for homes in India. Our plan is to use regression to predict the house price given the features found in the dataset and use $R^2$ and MSE to evaluate our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Among other metrics, house prices are oftentimes considered in evaluating the economic performance of a particular country. On a macro and micro level, housing prices can have significant consequences–ranging from an individual deciding to purchase their first home to evaluating a country’s Gross Domestic Product<a name=\"Congressional\"></a>[<sup>[1]</sup>](#Congressionalnote). Thus, given this importance, some study has been done into home price prediction.\n",
    "This importance has led to many attempts to predict house prices by researchers dating back to the 1990s.  The article, Predicting House Prices Using Multiple Listings Data, writes about the importance of accurately predicting house prices for its importance in administering mortgages and home owner insurance <a name=\"Dubin\"></a>[<sup>[2]</sup>](#Dubinnote) . To do so, the author discusses using three algorithms: Ordinary Least Squares, Maximum likelihood, and kriging. Noting that geographical location is a difficult variable to take into account using ordinary linear regression, the author delves into each algorithm and how it is implemented, considering what model is appropriate for each scenario; in particular, the author notes that the maximum likelihood function is useful for variables which are difficult to account for. The purpose behind this is to use the correlations between the prices of nearby homes to get a more accurate estimate of a given home’s price in a more effective method given the difficulties with OLS and geography. The article then discusses a practical example from listing with Baltimore, comparing the various techniques explored in the article. In particular, the article uses grid search to maximize the parameters of the likelihood function to provide the best prediction. While the $R^2$ score for the algorithms was around $0.7$, the author did see significant improvements (as much as 65.3% improvement) using methods to determine relative housing price from nearby houses over traditional OLS. A table of values comparing the estimation sample from the prediction samples show that the estimations are fairly accurate. However, given the age of the article, it would be interesting to see if such techniques still hold up today in the world of deep networks. \n",
    "Recent research attempts to use a generalized Linear Regression in conjunction with other statistical measures as a baseline model to predict housing prices. This is done to improve the accuracy of models that attempt to predict housing price with the importance housing prices have for both a consumer and supplier as each considers the risks involved in purchasing real estate<a name=\"li\"></a>[<sup>[3]</sup>](#Linote) . In particular, the article considers controlling investment in real estate as it has historically caused long-term economic issues citing this as a motivating factor for accurately predicting housing prices. The article delves into a detailed explanation of how their model is trained and all the different parameters it considers. Although the accuracy of the model is not stated, it seems that this group use of nonparametric parameters has led to a more accurate model compared to previous models.\n",
    "With respect to the dataset we are currently using, a handful of kaggle users have made attempts to predict house price given the dataset already, with limited success. There have been 15 notebooks made with a handful using ML with varying degrees of quality and accuracy. One of the more completed notebooks is<a name=\"Masghiff\"></a>[<sup>[4]</sup>](#Masghiffnote) . This notebook looked at 5 different models (Decision Trees, Random Forests, Gradient Boosting, Ridge CV, and ElasticNetCV) with onehot encoding and scaling transformations. There was no cross validation, model selection or hyper parameter tuning. The best scoring model  was random forests with a R^2 of 0.72 and an MSE of.0.27. Our work will focus much more on model improvements (and greater dataset expansion if possible) as a result to differentiate between us and previous work. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "Given the number of beds, baths, arces, city, state, zip code, and the size of a given house in cities in India, we want to predict the price of that house as closely to the actual price of that house as possible. This will be done using regression models (such as linear regression, random forests, etc) on the USA Real Estate Dataset Kaggle Dataset with performance measured by $R^2$ and MSE (and other potential evaluation meterics for regression based problems)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "UPDATED FROM PROPOSAL!\n",
    "\n",
    "You should have obtained and cleaned (if necessary) data you will use for this project.\n",
    "\n",
    "Please give the following infomration for each dataset you are using\n",
    "- link/reference to obtain it\n",
    "- description of the size of the dataset (# of variables, # of observations)\n",
    "- what an observation consists of\n",
    "- what some critical variables are, how they are represented\n",
    "- any special handling, transformations, cleaning, etc you have done should be demonstrated here!\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Area</th>\n",
       "      <th>Location</th>\n",
       "      <th>No. of Bedrooms</th>\n",
       "      <th>Resale</th>\n",
       "      <th>MaintenanceStaff</th>\n",
       "      <th>Gymnasium</th>\n",
       "      <th>SwimmingPool</th>\n",
       "      <th>LandscapedGardens</th>\n",
       "      <th>JoggingTrack</th>\n",
       "      <th>...</th>\n",
       "      <th>BED</th>\n",
       "      <th>VaastuCompliant</th>\n",
       "      <th>Microwave</th>\n",
       "      <th>GolfCourse</th>\n",
       "      <th>TV</th>\n",
       "      <th>DiningTable</th>\n",
       "      <th>Sofa</th>\n",
       "      <th>Wardrobe</th>\n",
       "      <th>Refrigerator</th>\n",
       "      <th>City</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30000000</td>\n",
       "      <td>3340</td>\n",
       "      <td>JP Nagar Phase 1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Bangalore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7888000</td>\n",
       "      <td>1045</td>\n",
       "      <td>Dasarahalli on Tumkur Road</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Bangalore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4866000</td>\n",
       "      <td>1179</td>\n",
       "      <td>Kannur on Thanisandra Main Road</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Bangalore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8358000</td>\n",
       "      <td>1675</td>\n",
       "      <td>Doddanekundi</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Bangalore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6845000</td>\n",
       "      <td>1670</td>\n",
       "      <td>Kengeri</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Bangalore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7714</th>\n",
       "      <td>14500000</td>\n",
       "      <td>1180</td>\n",
       "      <td>Mira Road East</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mumbai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7715</th>\n",
       "      <td>14500000</td>\n",
       "      <td>530</td>\n",
       "      <td>Naigaon East</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mumbai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7716</th>\n",
       "      <td>4100000</td>\n",
       "      <td>700</td>\n",
       "      <td>Shirgaon</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mumbai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7717</th>\n",
       "      <td>2750000</td>\n",
       "      <td>995</td>\n",
       "      <td>Mira Road East</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mumbai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7718</th>\n",
       "      <td>2750000</td>\n",
       "      <td>1020</td>\n",
       "      <td>Mira Road East</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mumbai</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32963 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Price  Area                         Location  No. of Bedrooms  \\\n",
       "0     30000000  3340                 JP Nagar Phase 1                4   \n",
       "1      7888000  1045       Dasarahalli on Tumkur Road                2   \n",
       "2      4866000  1179  Kannur on Thanisandra Main Road                2   \n",
       "3      8358000  1675                     Doddanekundi                3   \n",
       "4      6845000  1670                          Kengeri                3   \n",
       "...        ...   ...                              ...              ...   \n",
       "7714  14500000  1180                   Mira Road East                2   \n",
       "7715  14500000   530                     Naigaon East                1   \n",
       "7716   4100000   700                         Shirgaon                1   \n",
       "7717   2750000   995                   Mira Road East                2   \n",
       "7718   2750000  1020                   Mira Road East                2   \n",
       "\n",
       "      Resale  MaintenanceStaff  Gymnasium  SwimmingPool  LandscapedGardens  \\\n",
       "0          0               1.0        1.0           1.0                1.0   \n",
       "1          0               0.0        1.0           1.0                1.0   \n",
       "2          0               0.0        1.0           1.0                1.0   \n",
       "3          0               0.0        0.0           0.0                0.0   \n",
       "4          0               1.0        1.0           1.0                1.0   \n",
       "...      ...               ...        ...           ...                ...   \n",
       "7714       0               NaN        NaN           NaN                NaN   \n",
       "7715       1               NaN        NaN           NaN                NaN   \n",
       "7716       0               NaN        NaN           NaN                NaN   \n",
       "7717       0               NaN        NaN           NaN                NaN   \n",
       "7718       0               NaN        NaN           NaN                NaN   \n",
       "\n",
       "      JoggingTrack  ...  BED  VaastuCompliant  Microwave  GolfCourse   TV  \\\n",
       "0              1.0  ...  0.0              0.0        0.0         0.0  0.0   \n",
       "1              1.0  ...  0.0              1.0        0.0         0.0  0.0   \n",
       "2              1.0  ...  0.0              0.0        0.0         0.0  0.0   \n",
       "3              0.0  ...  0.0              0.0        0.0         0.0  0.0   \n",
       "4              1.0  ...  0.0              0.0        0.0         0.0  0.0   \n",
       "...            ...  ...  ...              ...        ...         ...  ...   \n",
       "7714           NaN  ...  NaN              NaN        NaN         NaN  NaN   \n",
       "7715           NaN  ...  NaN              NaN        NaN         NaN  NaN   \n",
       "7716           NaN  ...  NaN              NaN        NaN         NaN  NaN   \n",
       "7717           NaN  ...  NaN              NaN        NaN         NaN  NaN   \n",
       "7718           NaN  ...  NaN              NaN        NaN         NaN  NaN   \n",
       "\n",
       "      DiningTable  Sofa  Wardrobe  Refrigerator       City  \n",
       "0             0.0   0.0       0.0           0.0  Bangalore  \n",
       "1             0.0   0.0       0.0           0.0  Bangalore  \n",
       "2             0.0   0.0       0.0           0.0  Bangalore  \n",
       "3             0.0   0.0       0.0           0.0  Bangalore  \n",
       "4             0.0   0.0       0.0           0.0  Bangalore  \n",
       "...           ...   ...       ...           ...        ...  \n",
       "7714          NaN   NaN       NaN           NaN     Mumbai  \n",
       "7715          NaN   NaN       NaN           NaN     Mumbai  \n",
       "7716          NaN   NaN       NaN           NaN     Mumbai  \n",
       "7717          NaN   NaN       NaN           NaN     Mumbai  \n",
       "7718          NaN   NaN       NaN           NaN     Mumbai  \n",
       "\n",
       "[32963 rows x 41 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data Cleaning\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "#Data is saved on diffrent CSVs for each city\n",
    "#To make things easier, we can combine the data into one dataframe\n",
    "dfs = []\n",
    "for file in os.listdir(\"data\"):\n",
    "    df = pd.read_csv(os.path.join(\"data\", file))\n",
    "    df[\"City\"] = file.replace(\".csv\", \"\")\n",
    "    dfs.append(df)\n",
    "df = pd.concat(dfs)\n",
    "\n",
    "#As documented on kaggle, 9 implies that this information was not found for a home.\n",
    "#Therefore we replaced all 9s with np.nan as is standard for empty values\n",
    "\n",
    "temp = df[\"No. of Bedrooms\"].copy()\n",
    "df = df.applymap(lambda x: (np.nan if x == 9  else  x))\n",
    "df[\"No. of Bedrooms\"] = temp\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Price                  0.000000\n",
       "Area                   0.000000\n",
       "Location               0.000000\n",
       "No. of Bedrooms        0.000000\n",
       "Resale                 0.000000\n",
       "MaintenanceStaff       0.693808\n",
       "Gymnasium              0.693808\n",
       "SwimmingPool           0.693808\n",
       "LandscapedGardens      0.693808\n",
       "JoggingTrack           0.693808\n",
       "RainWaterHarvesting    0.693808\n",
       "IndoorGames            0.693808\n",
       "ShoppingMall           0.693808\n",
       "Intercom               0.693808\n",
       "SportsFacility         0.693808\n",
       "ATM                    0.693808\n",
       "ClubHouse              0.693808\n",
       "School                 0.693808\n",
       "24X7Security           0.693808\n",
       "PowerBackup            0.693808\n",
       "CarParking             0.693808\n",
       "StaffQuarter           0.693808\n",
       "Cafeteria              0.693808\n",
       "MultipurposeRoom       0.693808\n",
       "Hospital               0.693808\n",
       "WashingMachine         0.693808\n",
       "Gasconnection          0.693808\n",
       "AC                     0.693808\n",
       "Wifi                   0.693808\n",
       "Children'splayarea     0.693808\n",
       "LiftAvailable          0.693808\n",
       "BED                    0.693808\n",
       "VaastuCompliant        0.693808\n",
       "Microwave              0.693808\n",
       "GolfCourse             0.693808\n",
       "TV                     0.693808\n",
       "DiningTable            0.693808\n",
       "Sofa                   0.693808\n",
       "Wardrobe               0.693808\n",
       "Refrigerator           0.693808\n",
       "City                   0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#A quick check for missinginess shows that the distrubtion for all nan values is equal accross the columns\n",
    "df.isnull().sum()/df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Area</th>\n",
       "      <th>Location</th>\n",
       "      <th>No. of Bedrooms</th>\n",
       "      <th>Resale</th>\n",
       "      <th>MaintenanceStaff</th>\n",
       "      <th>Gymnasium</th>\n",
       "      <th>SwimmingPool</th>\n",
       "      <th>LandscapedGardens</th>\n",
       "      <th>JoggingTrack</th>\n",
       "      <th>...</th>\n",
       "      <th>BED</th>\n",
       "      <th>VaastuCompliant</th>\n",
       "      <th>Microwave</th>\n",
       "      <th>GolfCourse</th>\n",
       "      <th>TV</th>\n",
       "      <th>DiningTable</th>\n",
       "      <th>Sofa</th>\n",
       "      <th>Wardrobe</th>\n",
       "      <th>Refrigerator</th>\n",
       "      <th>City</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30000000</td>\n",
       "      <td>3340</td>\n",
       "      <td>JP Nagar Phase 1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Bangalore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7888000</td>\n",
       "      <td>1045</td>\n",
       "      <td>Dasarahalli on Tumkur Road</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Bangalore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4866000</td>\n",
       "      <td>1179</td>\n",
       "      <td>Kannur on Thanisandra Main Road</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Bangalore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8358000</td>\n",
       "      <td>1675</td>\n",
       "      <td>Doddanekundi</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Bangalore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6845000</td>\n",
       "      <td>1670</td>\n",
       "      <td>Kengeri</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Bangalore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1393</th>\n",
       "      <td>62000000</td>\n",
       "      <td>1450</td>\n",
       "      <td>Worli</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Mumbai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1394</th>\n",
       "      <td>2500000</td>\n",
       "      <td>540</td>\n",
       "      <td>Virar East</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Mumbai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1395</th>\n",
       "      <td>19000000</td>\n",
       "      <td>1267</td>\n",
       "      <td>Belapur</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Mumbai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396</th>\n",
       "      <td>14900000</td>\n",
       "      <td>1245</td>\n",
       "      <td>Airoli</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Mumbai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1397</th>\n",
       "      <td>14000000</td>\n",
       "      <td>1183</td>\n",
       "      <td>Airoli</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Mumbai</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10093 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Price  Area                         Location  No. of Bedrooms  \\\n",
       "0     30000000  3340                 JP Nagar Phase 1                4   \n",
       "1      7888000  1045       Dasarahalli on Tumkur Road                2   \n",
       "2      4866000  1179  Kannur on Thanisandra Main Road                2   \n",
       "3      8358000  1675                     Doddanekundi                3   \n",
       "4      6845000  1670                          Kengeri                3   \n",
       "...        ...   ...                              ...              ...   \n",
       "1393  62000000  1450                            Worli                3   \n",
       "1394   2500000   540                       Virar East                1   \n",
       "1395  19000000  1267                          Belapur                3   \n",
       "1396  14900000  1245                           Airoli                2   \n",
       "1397  14000000  1183                           Airoli                2   \n",
       "\n",
       "      Resale  MaintenanceStaff  Gymnasium  SwimmingPool  LandscapedGardens  \\\n",
       "0          0               1.0        1.0           1.0                1.0   \n",
       "1          0               0.0        1.0           1.0                1.0   \n",
       "2          0               0.0        1.0           1.0                1.0   \n",
       "3          0               0.0        0.0           0.0                0.0   \n",
       "4          0               1.0        1.0           1.0                1.0   \n",
       "...      ...               ...        ...           ...                ...   \n",
       "1393       0               1.0        1.0           1.0                1.0   \n",
       "1394       0               0.0        0.0           0.0                0.0   \n",
       "1395       1               0.0        1.0           1.0                1.0   \n",
       "1396       0               0.0        0.0           0.0                0.0   \n",
       "1397       0               0.0        1.0           1.0                1.0   \n",
       "\n",
       "      JoggingTrack  ...  BED  VaastuCompliant  Microwave  GolfCourse   TV  \\\n",
       "0              1.0  ...  0.0              0.0        0.0         0.0  0.0   \n",
       "1              1.0  ...  0.0              1.0        0.0         0.0  0.0   \n",
       "2              1.0  ...  0.0              0.0        0.0         0.0  0.0   \n",
       "3              0.0  ...  0.0              0.0        0.0         0.0  0.0   \n",
       "4              1.0  ...  0.0              0.0        0.0         0.0  0.0   \n",
       "...            ...  ...  ...              ...        ...         ...  ...   \n",
       "1393           1.0  ...  0.0              1.0        0.0         1.0  0.0   \n",
       "1394           0.0  ...  0.0              0.0        0.0         0.0  0.0   \n",
       "1395           1.0  ...  0.0              1.0        0.0         0.0  0.0   \n",
       "1396           0.0  ...  0.0              0.0        0.0         0.0  0.0   \n",
       "1397           1.0  ...  0.0              1.0        0.0         0.0  0.0   \n",
       "\n",
       "      DiningTable  Sofa  Wardrobe  Refrigerator       City  \n",
       "0             0.0   0.0       0.0           0.0  Bangalore  \n",
       "1             0.0   0.0       0.0           0.0  Bangalore  \n",
       "2             0.0   0.0       0.0           0.0  Bangalore  \n",
       "3             0.0   0.0       0.0           0.0  Bangalore  \n",
       "4             0.0   0.0       0.0           0.0  Bangalore  \n",
       "...           ...   ...       ...           ...        ...  \n",
       "1393          0.0   0.0       0.0           0.0     Mumbai  \n",
       "1394          0.0   0.0       0.0           0.0     Mumbai  \n",
       "1395          0.0   0.0       0.0           0.0     Mumbai  \n",
       "1396          0.0   0.0       0.0           0.0     Mumbai  \n",
       "1397          0.0   0.0       0.0           0.0     Mumbai  \n",
       "\n",
       "[10093 rows x 41 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Given this implies that a given row with a nan values likely contains nan values, we can simply drop all rows with nan\n",
    "cleaned_df = df[~df.isnull().any(axis=1)]\n",
    "cleaned_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "This problem is a regression problem: Given some input features relating to a house, we predict some real valued output that is the price of that house. Therefore our proposed solution will find some optimal model for predicting housing prices.To achieve this, we will start by using various techniques and models with minimal error. The steps involved in our solution are as follows:\n",
    "\n",
    "Step 1 Data processing: We will process the housing data and handle missing values by filling them  in, doing some scaling, and encoding categorical variables if needed. This will help make sure that the data will be easily analyzed. Specifically we will at least attempt:\n",
    "\n",
    "- Scale: It may be appropriate to scale say geographical distances on a scale [0,1]. We are not sure yet how to scale but know based on our research that it may come in hand when considering geographical distances. Also, it may be useful to scale when considering similarity between housing which may be useful in predicting price. As for other features such as arce size and house size, logrithmic scaling may be appropriate to get better senses of magnitude. \n",
    "- Onehot encoding - it is possible to have categorical variables which in terms of linear regression would require one hot encoding\n",
    "\n",
    "Step 2 Model Selection: we will use all of the different regression models that we covered in class, and from there find the optimal one. These models may include:\n",
    "\n",
    "- Linear Regression:  A linear regression model seems appropriate given that we want to predict house price, say Y, given a dataset of houses and their respective attributes, say X.  In other words, given some input features relating to a house, we predict some real valued output that is the price of that house. Thus we can model this problem as $Y = Xw$\n",
    "- random forests: Each decision tree in the random forest guesses some prediction for the house price based on a random sampling of houses and features. Then we can take the mean guess of those random forests and use that as our prediction score. \n",
    "- gradient boosting: When we are training our model it is probable that there will be some sort of misclassification so it will be necessary that we take into account the error and focus on correcting it on the next iteration. Also, we are not guaranteed a smooth continuous function for our data which may require gradient descent so that we can choose our optimal value, which in this case will be the most accurate prediction.\n",
    "- Neural Network: Vectorize all the data, that will be our input. Each layer of the neural network will transform and condense the input data. The last layer of the neural network will be a single output representing some number. We can then train our model by attempting to minimize MSE between the output value and the real price of the house.\n",
    "- Other possible alternatives: Based on background research, we have noticed that regression based ML systems may struggle with understanding geologically important information. Thus we may want to look into possible methods that can handle geographic data.  \n",
    "\n",
    "Step 3 Cross Validation: After gathering the data with the models and given the size of the dataset, we can use a train test split (80/20) and cross validation for model selection.\n",
    "\n",
    "Step 4 Hyperparameter Tuning: Using techniques such as grid search, we can find out the best set of hyperparameters to be used for each model and optimize their performance.\n",
    "\n",
    "Step 5: Model Selection: The optimal model to be used to predict and be trained on will be found based on getting the lowest MSE, MAE, or highest R-Squared value. From there we have our machine learning predictor\n",
    "\n",
    "Much of this work can be easily implemented in sklearn using pandas to hold the data, numpy for extra transformations if needed, and pytorch for any neural network based model we are interested in trying out. \n",
    "\n",
    "A baseline model for this dataset, as previously discussed in the background section is a kaggle notebook which did our problem on this dataset at <a name=\"Masghiff\"></a>[<sup>[4]</sup>](#Masghiffnote). Given the original dataset did not do any kind of cross validation to conduct model selection, We can use thier pipeline for transforming the data in addition to the best model they discovered to compare against our techniques. Even if we attempt to expand the dataset via web scraping, we can still use that model for a baseline model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "\n",
    "**Mean Squared Error (MSE)**  \n",
    "MSE is defined as: $$MSE = \\frac{1}{n} \\sum_{i=1}^{n}(y_i - \\hat{y_i})^2$$\n",
    "This metric measures the average of the squared differences between predicted and actual values. This in an appropriate metric because it provides us a measurement of how accurate our predictions are with respect to the actual values. Based on our MSE value, we can quantify how close or far we are when we compare our predicted values to acutal values. The lower the MSE, the better the model is at fitting the data. \n",
    "  \n",
    "**R-squared ($R^2$)**    \n",
    "R^2 is defined as:$$R^2 = 1 - \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y_i})^2}{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}$$  \n",
    "This metric measures the fit of a regression model. This value ranges from 0 to 1. An R^2 value of 1 implies a perfect fit while a value of 0 there is no fit or relationship between the dependent and indepedent variables. This is an appropriate metric because it allows us to quantify the variance in the dependent variable in relation to the independent variable. Additionally, we can understand the fit of our data, meaning whether our data is overfit or underfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Given a possible pipeline, get the score metrics for it\n",
    "def score_suite(pipe, X_test, y_test):\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    return (\n",
    "        pipe.score(X_test, y_test),\n",
    "        sklearn.metrics.mean_squared_error(y_test, y_pred),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary results\n",
    "\n",
    "NEW SECTION!\n",
    "\n",
    "Please show any preliminary results you have managed to obtain.\n",
    "\n",
    "Examples would include:\n",
    "- Analyzing the suitability of a dataset or alogrithm for prediction/solving your problem \n",
    "- Performing feature selection or hand-designing features from the raw data. Describe the features available/created and/or show the code for selection/creation\n",
    "- Showing the performance of a base model/hyper-parameter setting.  Solve the task with one \"default\" algorithm and characterize the performance level of that base model.\n",
    "- Learning curves or validation curves for a particular model\n",
    "- Tables/graphs showing the performance of different models/hyper-parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotly.figure_factory as ff\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Price', 'Area', 'Location', 'No. of Bedrooms', 'Resale',\n",
       "       'MaintenanceStaff', 'Gymnasium', 'SwimmingPool',\n",
       "       'LandscapedGardens', 'JoggingTrack', 'RainWaterHarvesting',\n",
       "       'IndoorGames', 'ShoppingMall', 'Intercom', 'SportsFacility', 'ATM',\n",
       "       'ClubHouse', 'School', '24X7Security', 'PowerBackup', 'CarParking',\n",
       "       'StaffQuarter', 'Cafeteria', 'MultipurposeRoom', 'Hospital',\n",
       "       'WashingMachine', 'Gasconnection', 'AC', 'Wifi',\n",
       "       \"Children'splayarea\", 'LiftAvailable', 'BED', 'VaastuCompliant',\n",
       "       'Microwave', 'GolfCourse', 'TV', 'DiningTable', 'Sofa', 'Wardrobe',\n",
       "       'Refrigerator', 'City'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1776"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[\"Location\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distrubtions of all columns as box blots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cleaned_df:\n",
    "    if (cleaned_df[col].dtype != object):\n",
    "        print(col)\n",
    "        seaborn.boxplot(data=cleaned_df[col].values)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we illustrate the number of houses from each city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=cleaned_df['City'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a piechart we can break down the distribution the percentage of homes within each city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = plt.figure(figsize =(10, 7))\n",
    "cities =['Hyderabad','Chennai','Delhi','Bangalore','Mumbai','Kolkata']\n",
    "plt.pie(cleaned_df['City'].value_counts(), labels=cities, autopct='%.1f%%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We visualize each city's average price per home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = cleaned_df.groupby('City')['Price'].mean().plot(kind='bar')\n",
    "ax.set_ylabel(\"average price\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We determine the strongest correlation between price,number of bedrooms, and area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_to_consider = ['Price','No. of Bedrooms','Area']\n",
    "corr = cleaned_df[vars_to_consider].corr()\n",
    "sns.heatmap(corr, annot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we try to find which ammenities have the strongest correlation with price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ammenities = ['MaintenanceStaff', 'Gymnasium', 'SwimmingPool', 'LandscapedGardens',\n",
    "       'JoggingTrack', 'RainWaterHarvesting', 'IndoorGames', 'ShoppingMall',\n",
    "       'Intercom', 'SportsFacility', 'ATM', 'ClubHouse', 'School',\n",
    "       '24X7Security', 'PowerBackup', 'CarParking', 'StaffQuarter',\n",
    "       'Cafeteria', 'MultipurposeRoom', 'Hospital', 'WashingMachine',\n",
    "       'Gasconnection', 'AC', 'Wifi', \"Children'splayarea\", 'LiftAvailable',\n",
    "       'BED', 'Microwave', 'GolfCourse', 'TV',\n",
    "       'DiningTable', 'Sofa', 'Wardrobe', 'Refrigerator']\n",
    "\n",
    "corr_price_ammenities = {}\n",
    "for ele in ammenities:\n",
    "    key = \"Price and \" + ele\n",
    "    corr = cleaned_df['Price'].corr(df[ele])\n",
    "    corr_price_ammenities[key] = corr\n",
    "\n",
    "sorted_dict = sorted(corr_price_ammenities.items(), key = lambda x:x[1], reverse = True)\n",
    "new_df  = pd.Series(sorted_dict).to_frame()\n",
    "new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ETC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ff' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-11c1b5516af9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mhist_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcleaned_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcleaned_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'City'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mcity\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Price'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m fig = ff.create_distplot(\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mhist_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhist_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mgroup_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcity_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ff' is not defined"
     ]
    }
   ],
   "source": [
    "city_list = []\n",
    "hist_list = []\n",
    "for city in cleaned_df['City'].unique():\n",
    "    city_list.append(city)\n",
    "    hist_list.append(cleaned_df.loc[cleaned_df['City'] == city, 'Price'])\n",
    "\n",
    "fig = ff.create_distplot(\n",
    "    hist_data = hist_list,\n",
    "    group_labels = city_list,\n",
    "    show_rug = False,\n",
    "    show_hist = False,\n",
    ")\n",
    "\n",
    "fig.update_xaxes(range=[0, 50000000])\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = cleaned_df.groupby('City')\n",
    "\n",
    "for city, data in grouped_df:\n",
    "    plt.scatter(data['Area'], data['Price']/1e8, label=city)\n",
    "\n",
    "plt.xlabel('Area')\n",
    "plt.ylabel('Price')\n",
    "plt.title('Scatterplot of Area vs. Price for Each City in Millions')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "train = pd.read_csv(os.path.join(\"data\", \"Bangalore.csv\"))\n",
    "train.columns.values\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "train = pd.read_csv(os.path.join(\"data\", \"Chennai.csv\"))\n",
    "train.columns.values\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "dfs = []\n",
    "for file in os.listdir(\"data\"):\n",
    "    df = pd.read_csv(os.path.join(\"data\", file))\n",
    "    df[\"City\"] = file.replace(\".csv\", \"\")\n",
    "    dfs.append(df)\n",
    "df = pd.concat(dfs)\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "len(np.unique(col))\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "df\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "df.to_csv(\"india_homes.csv\")\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "df = df[~(df == 9).any(axis=1)]\n",
    "len(df)\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "cities = df['City'].unique()\n",
    "cities\n",
    "\n",
    "\n",
    "# In[64]:\n",
    "\n",
    "\n",
    "# List of cities\n",
    "cities = ['Kolkata', 'Delhi', 'Chennai', 'Bangalore', 'Hyderabad', 'Mumbai']\n",
    "\n",
    "# Create an empty list to store the price data for each city\n",
    "city_prices = []\n",
    "\n",
    "# Iterate over each city\n",
    "for city in cities:\n",
    "    # Filter the prices of homes for the current city\n",
    "    prices = df[df['City'] == city]['Price']\n",
    "    \n",
    "    # Add the prices to the city_prices list\n",
    "    city_prices.append(prices)\n",
    "\n",
    "# Create the box plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot(city_prices, labels=cities)\n",
    "plt.title('Price Distribution by City')\n",
    "plt.xlabel('City')\n",
    "plt.ylabel('Price')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[56]:\n",
    "\n",
    "\n",
    "# Create subplots for histograms\n",
    "fig, axs = plt.subplots(1, 4, figsize=(12, 4))\n",
    "\n",
    "# Plot histogram for Price\n",
    "axs[0].hist(df['Price'], bins=10, edgecolor='k', color='#20A5DF')\n",
    "axs[0].set_title('Price')\n",
    "axs[0].set_xlabel('Price')\n",
    "axs[0].set_ylabel('Frequency')\n",
    "\n",
    "# Plot histogram for Area\n",
    "axs[1].hist(df['Area'], bins=10, edgecolor='k', color='#45DF20')\n",
    "axs[1].set_title('Area')\n",
    "axs[1].set_xlabel('Area')\n",
    "axs[1].set_ylabel('Frequency')\n",
    "\n",
    "# Plot histogram for No. of Bedrooms\n",
    "axs[2].hist(df['No. of Bedrooms'], bins=10, edgecolor='k', color='#DF5A20')\n",
    "axs[2].set_title('Bedrooms')\n",
    "axs[2].set_xlabel('No. of Bedrooms')\n",
    "axs[2].set_ylabel('Frequency')\n",
    "\n",
    "# Compute No. of Amenities for each row\n",
    "sum_values = df.loc[:, 'MaintenanceStaff':'Refrigerator'].sum(axis=1)\n",
    "\n",
    "# Plot histogram for No. of Amenities\n",
    "axs[3].hist(sum_values, bins=10, edgecolor='k', color='#BA20DF')\n",
    "axs[3].set_title('Amenities')\n",
    "axs[3].set_xlabel('No. of Amenities')\n",
    "axs[3].set_ylabel('Frequency')\n",
    "\n",
    "# Plot Title\n",
    "fig.suptitle('Histograms of Feature Distribution ', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the histograms\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[62]:\n",
    "\n",
    "\n",
    "# Compute the sum of columns from MaintenanceStaff to Refrigerator\n",
    "sum_values = df.loc[:, 'MaintenanceStaff':'Refrigerator'].sum(axis=1)\n",
    "\n",
    "# Create a new DataFrame with the sum of amenities included\n",
    "correlation_df = df.assign(**{'Sum of Amenities': sum_values})\n",
    "\n",
    "# Select the numerical columns for correlation analysis\n",
    "numerical_columns = ['Price', 'Area', 'No. of Bedrooms', 'Sum of Amenities']\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = correlation_df[numerical_columns].corr()\n",
    "\n",
    "# Create a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', square=True)\n",
    "\n",
    "# Rotate x-axis tick labels\n",
    "plt.xticks(rotation=25)\n",
    "\n",
    "# Set the title and display the plot\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#=============================================================================\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "grouped_df = cleaned_df.groupby('City')\n",
    "\n",
    "\n",
    "for city, data in grouped_df:\n",
    "    plt.scatter(data['Area'], data['Price']/1e8, label=city)\n",
    "\n",
    "plt.xlabel('Area')\n",
    "plt.ylabel('Price')\n",
    "plt.title('Scatterplot of Area vs. Price for Each City in Millions')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_list = []\n",
    "hist_list = []\n",
    "for city in cleaned_df['City'].unique():\n",
    "    city_list.append(city)\n",
    "    hist_list.append(cleaned_df.loc[cleaned_df['City'] == city, 'Price'])\n",
    "\n",
    "fig = ff.create_distplot(\n",
    "    hist_data = hist_list,\n",
    "    group_labels = city_list,\n",
    "    show_rug = False,\n",
    "    show_hist = False,\n",
    "\n",
    "\n",
    ")\n",
    "\n",
    "fig.update_xaxes(range=[0, 50000000])\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = ColumnTransformer(\n",
    "    [\n",
    "        (\"Location_One_Hot\",  OneHotEncoder(handle_unknown=\"ignore\"), [\"Location\", \"City\"]),\n",
    "    ],\n",
    "    remainder=\"passthrough\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ct' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-b468685e8c36>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpipe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'transformers'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mct\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'svc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mpipe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ct' is not defined"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([('transformers', ct), ('svc', LinearRegression())])\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    cleaned_df.drop(\"Price\", axis=1), \n",
    "    cleaned_df[\"Price\"], \n",
    "    test_size=0.33, \n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_suite(pipe, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three primarily ethics issues with our dataset 1) Lack of Informed Consent and 2) Possible collection and/or dataset bias and 3) issues with future generalizablity of the model. \n",
    "\n",
    "Since the data we are planning to use is web scraped data from publicly available websites, homeowners are unlikely to be aware of our project. Thus the rough description of their home may be included in our data analysis similar to the information available to anyone with access to realtor.com or zillow.com. Founrelty, we are unable to determine who current homeowners are with the data we have, so all other elements of a homeowners identity are hidden. However, given this we will be unable to contact these homeowners and be able to gain consent to use data about thier house. Thus leading to a lack of informed consent regrauding our project. The best solution for this issue is to allow people to message us online via github where this project is located and raise an issue with the data so we may remove their house from the analysis. \n",
    "\n",
    "We may also have a collection and/or dataset bias if we choose to use the current kaggle dataset. While the kaggle dataset advertises itself as relating to the whole USA, it is limited to only Massachusetts and Puerto Rico. This means if we wish to keep the scope of the project to the entire USA, we will be biasing our model by only considering these 2 states/terrtories. The best remedy we have for this is to gather more data, mostly likely by building our own webscraper to gather information about the houses on realtor.com (which the kaggle dataset scraped their data from) or by getting in contact with the original creator of the dataset and seeing if a more encompassing dataset can be produced. That way the location of the house can more accurately and ethically predict the price of the house. \n",
    "\n",
    "Finally there is the potential for users to be harmed by this model in the future as housing prices can change with the housing market, policy decisions etc that the model will not be able to account for given the data we have. If a user in 10 years wishes to use the model to predict what their house price will be, it has the potential to be very wrong if there is a large change in the housing market of a particular area. If that user makes a decision on that information it will likely harm their ability to search for a good house. Thus the model will likely need a pipeline to continuously update itself given a new market if put into production. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hold in-person meetings to discuss ideas and progress\n",
    "* Communication will be handled remotely through Discord\n",
    "* Ensure minimum consensus of 3/4 members when making decisions\n",
    "* Expected individual contribution of at least 25% for each checkpoint/deliverable\n",
    "* Individual tasks will have earlier deadlines prior to submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UPDATE THE PROPOSAL TIMELINE ACCORDING TO WHAT HAS ACTUALLY HAPPENED AND HOW IT HAS EFFECTED YOUR FUTURE PLANS\n",
    "\n",
    "\n",
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 5/12 | 4 PM | Determine best form of communication |  Discuss each individuals interests to find a topic for the project|\n",
    "| 5/17  | Before 11:59 PM |  Look for interesting datasets; finalize Research Proposal | Webscraping and additional data plans |\n",
    "| 5/20  |  12 PM | Finalize Web Scraper If Used | Discuss EDA plan |\n",
    "| 5/25  |  12 PM | EDA Finished | Begin planing Baseline Model |\n",
    "| 5/30  |  12 PM | Baseline Model Finshed | Prep checkpoint notebook for submission |\n",
    "| 5/31  |   Before 11:59 PM |  Checkpoint Due | Discuss model selection plans |\n",
    "| 6/10  |  12 PM | Finalize Model Selection | Plan out remaining tasks | \n",
    "| 6/14  |   Before 11:59 PM |  Project Due | NA |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"Congressionalnote\"></a>1.[^](#Congressional) Congressional Research Service. (2023, January 3). Introduction to U.S. economy: Housing market - federation of American ... Introduction to U.S. Economy: Housing Market. https://sgp.fas.org/crs/misc/IF11327.pdf\n",
    "\n",
    "<a name=\"Dubinnote\"></a>2.[^](#Dubin)  Dubin, R. A. (1998). Predicting House Prices Using Multiple Listings Data. The Journal of Real Estate Finance and Economics, 17(1), 35–59. https://doi.org/10.1023/a:1007751112669\n",
    "\n",
    "<a name=\"Linote\"></a>3.[^](#li)  Li, X. (2022). Prediction and analysis of housing price based on the generalized linear regression model. Computational Intelligence and Neuroscience, 2022, 1–9. https://doi.org/10.1155/2022/3590224\n",
    "\n",
    "<a name=\"Masghiffnote\"></a>4.[^](#Masghiff) Masghiff. (2023, May 5). Predicting housing prices EDA + ml. Kaggle. https://www.kaggle.com/code/masghiff/predicting-housing-prices-eda-ml "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
